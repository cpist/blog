<!DOCTYPE html><!--TI0pD73CnhalHln6RXV_z--><html lang="ko"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/blog/_next/static/chunks/d335db6b3f1a2619.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/blog/_next/static/chunks/2139e000f4b5d584.js"/><script src="/blog/_next/static/chunks/8a8ef77865bda9e6.js" async=""></script><script src="/blog/_next/static/chunks/0ff423a9fcc0186e.js" async=""></script><script src="/blog/_next/static/chunks/88a8688d62cd2814.js" async=""></script><script src="/blog/_next/static/chunks/turbopack-566b9f8f22ac84c4.js" async=""></script><script src="/blog/_next/static/chunks/796e69ae18b2784c.js" async=""></script><script src="/blog/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/blog/_next/static/chunks/247eb132b7f7b574.js" async=""></script><script src="/blog/_next/static/chunks/631eeae4923b8465.js" async=""></script><title>Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations - CPIST&#x27;s blog</title><meta name="description" content="Structure of Triton"/><link rel="icon" href="/blog/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/blog/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><main><header style="display:flex;justify-content:space-between;align-items:baseline;gap:12px"><h1 style="margin:8px 0"><a style="text-decoration:none" href="/blog/">CPIST&#x27;s blog</a></h1><nav style="display:flex;gap:12px"><a href="/blog/posts/">Posts</a></nav></header><hr style="border:0;border-top:1px solid #eee;margin:12px 0 24px"/><article><div style="color:#666;margin-bottom:6px"><a href="/paper" style="text-decoration:none">/<!-- -->paper</a></div><h2 style="margin-top:0">Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</h2><div style="color:#666;margin-bottom:18px">2026-01-04</div><p style="color:#333">Structure of Triton</p><hr style="border:0;border-top:1px solid #eee;margin:18px 0"/><div class="prose"><p>Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations (2019) 는 MAPL 워크숍에서 발표된 논문임.
Triton 의 구조에 대해 다루고 있음</p>
<p>1) Triton Architecture Overview
Triton의 아키텍처는 크게 3단계로 나뉜다.</p>
<ol>
<li>
<p>Triton-C (Frontend)</p>
<ul>
<li>Tile Semantics: ANSI C(CUDA-C) 문법을 따르지만, int tile[16, 16]과 같이 **Tile(다차원 배열)**을 일급 객체(First-class citizen)로 취급</li>
<li>Broadcasting: NumPy와 유사한 브로드캐스팅 규칙을 언어 차원에서 지원하여 차원이 다른 텐서 간의 연산을 직관적으로 표현</li>
<li>SPMD on Blocks: CUDA와 달리 커널이 Single-threaded로 작성되지만<!-- -->
<ul>
<li>각 인스턴스가 자동으로 병렬화되어 Global Range(Grid)의 특정 타일을 처리</li>
<li>개발자는 스레드 간 동기화(Shared memory sync)를 직접 신경 쓸 필요가 없음</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Triton-IR (Middle-end)</p>
<ul>
<li>LLVM-based: LLVM-IR과 구조가 유사하지만 타일 레벨의 데이터 흐름(Data-flow)과 제어 흐름(Control-flow) 분석을 위한 확장이 포함됨</li>
<li>Tile Instructions: reshape, broadcast, dot, trans 같은 타일 전용 명령어가 추가됨</li>
<li>Predication: 타일 내부의 개별 요소에 대한 분기 처리를 위해 Predicated SSA (PSSA) 형식을 사용하여 제어 흐름을 표현</li>
</ul>
</li>
<li>
<p>Triton-JIT (Backend)</p>
<ul>
<li>Triton-IR을 입력받아 최적화된 머신 코드(PTX/LLVM bitcode)를 생성</li>
<li>Auto-tuner가 내장되어 있어 타일 크기 등의 메타 파라미터를 자동으로 최적화</li>
</ul>
</li>
</ol>
<p>2) Key Compiler Optimizations
Triton-JIT의 핵심 최적화 패스(Pass)들은 다음과 같음.</p>
<ol>
<li>
<p>Machine-Independent Passes</p>
<ul>
<li>Prefetching: 루프 내에서 타일 레벨 메모리 연산으로 인한 지연(Latency)을 숨기고자 함<!-- -->
<ul>
<li>컴파일러가 루프를 감지하고 적절한 프리패칭(Pre-fetching) 코드를 삽입</li>
<li>Peephole Optimization: 타일 대수(Algebra) 속성을 이용해 Trans(Trans(X)) == X와 같은 패턴을 감지하고 단순화</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Machine-Dependent Passes (GPU Targeting)</p>
<ul>
<li>Hierarchical Tiling (계층적 타일링)<!-- -->
<ul>
<li>타일을 Micro-tile, Nano-tile로 분해하여 GPU의 메모리 계층 구조(DRAM <code>-&gt;</code> L2 <code>-&gt;</code> L1/Shared <code>-&gt;</code> Register)에 딱 맞게 매핑</li>
<li>Polyhedral 모델 없이도 중첩된 타일링 구성을 자동으로 열거하고 최적화 가능</li>
</ul>
</li>
<li>Memory Coalescing (메모리 결합)<!-- -->
<ul>
<li>Triton-IR은 논리적으로 Single-threaded이지만<!-- -->
<ul>
<li>백엔드에서 이를 병렬화할 때 인접한 스레드가 인접한 메모리 주소에 접근하도록 스레드 순서를 재배치</li>
</ul>
</li>
<li>이를 통해 DRAM 트랜잭션 효율을 극대화</li>
</ul>
</li>
<li>Shared Memory Allocation (공유 메모리 할당)<!-- -->
<ul>
<li>dot 연산처럼 연산 강도(Arithmetic Intensity)가 높은 작업의 피연산자를 Shared Memory에 저장</li>
<li>변수의 Live Range를 분석하여 선형 시간(Linear-time) 스토리지 할당 알고리즘을 사용해 최적의 위치를 결정</li>
</ul>
</li>
<li>Automatic Barrier Insertion (자동 동기화)<!-- -->
<ul>
<li>RAW(Read-After-Write) 및 WAR(Write-After-Read) 해저드를 감지하기 위해 데이터 흐름 분석(Data-flow analysis)을 수행</li>
<li>필요한 지점에 자동으로 Barrier를 삽입하여 프로그램의 정확성을 보장</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>3) Performance Benchmarks</p>
<ul>
<li>Matrix Multiplication: DeepSpeech2, Transformer 등의 워크로드에서 cuBLAS와 대등한 성능을 보임<!-- -->
<ul>
<li>또한 타 DSL(Halide, TVM, PlaidML 등) 대비 최대 2-3배 빠른 성능을 기록</li>
</ul>
</li>
<li>Convolutions: cuDNN의 IMPLICIT_GEMM 알고리즘을 재구현했을 때 성능 저하가 없음<!-- -->
<ul>
<li>특히 ResNet 일부 태스크에서는 cuDNN보다 뛰어난 성능을 보임</li>
</ul>
</li>
<li>Shift-Convolutions: 기존에는 핸드 튜닝된 커널과 cuBLAS 호출을 섞어 써야 했던 Shift-Conv를 Triton-C로 하나의 융합된(Fused) 커널로 구현<!-- -->
<ul>
<li>Shift 연산 비용을 거의 완벽하게 숨기는 성능을 달성</li>
</ul>
</li>
</ul>
<p>4) 엔지니어로서의 포인트</p>
<ul>
<li>Tile Semantics &amp; Broadcasting<!-- -->
<ul>
<li>기존의 스칼라 기반 루프 최적화(Loop transformation) 접근법 대신, Tile 자체를 타입으로 정의하고 연산 단위로 삼는 것이 하드웨어(Tensor Core 등) 활용에 얼마나 유리한지를 확인</li>
<li>Broadcasting 규칙을 IR 레벨에서 지원함으로써 차원 불일치 문제를 우아하게 해결하고 벡터화(Vectorization) 기회를 명시적으로 드러냄</li>
</ul>
</li>
<li>IR Design: Predicated SSA<!-- -->
<ul>
<li>GPU와 같은 SIMT 구조를 컴파일할 때 가장 까다로운 것 중 하나가 Divergent Control Flow</li>
<li>Triton은 타일 내부의 제어 흐름을 분기(Branch)가 아닌 Predicated SSA 형식(psi 명령어 등)으로 처리</li>
<li>벡터화된 실행 흐름을 유지하면서도 조건부 연산을 효율적으로 표현</li>
<li>이는 마스킹(Masking)을 통한 벡터 연산 최적화의 핵심이자, 배울 부분</li>
</ul>
</li>
<li>Automatic Synchronization<!-- -->
<ul>
<li>CUDA 프로그래밍의 가장 큰 진입 장벽인 __syncthreads() 수동 관리를 컴파일러 패스로 해결</li>
<li>**데이터 흐름 분석(Liveness Analysis)**을 통해 공유 메모리 접근 간의 의존성을 파악하고 Barrier를 자동 삽입<!-- -->
<ul>
<li>이건 고성능 병렬 컴파일러의 핵심적으로 다루어야 할 부분이라 할 수 있음</li>
</ul>
</li>
</ul>
</li>
<li>Optimization Space Extraction<!-- -->
<ul>
<li>Auto-tuner가 단순히 미리 정의된 템플릿을 튜닝하는 것이 아니라, IR에서 직접 최적화 공간(타일 사이즈, 계층 구조 등)을 추출하여 탐색<!-- -->
<ul>
<li>컴파일러 기반 오토튜닝의 이상적인 방향성을 보여줬다고도 할 수 있다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>===============================================</p>
<p>이 논문을 요약해 보자면 왜 Triton을 구현했고, 어떻게 구현했는가에 포커싱 되어 있음.</p>
<ul>
<li>
<p>딥러닝 분야에서 새로운 아이디어를 검증하고 배포할 때, 효율적인 연산 커널(Compute Kernel)의 부재가 병목이 되는 경우가 많음</p>
<ul>
<li>cuBLAS나 cuDNN 같은 벤더 라이브러리는 성능이 뛰어나지만 제한된 연산만 지원하며</li>
<li>TVM이나 PlaidML 같은 DSL은 유연하지만 벤더 라이브러리만큼의 성능을 내기 어려움
Triton은 이러한 문제를 해결하기 위해 제안된 언어이자 컴파일러이다.</li>
</ul>
</li>
<li>
<p>여기서 핵심 아이디어는 &quot;Tile(타일)&quot; 개념을 중심으로 한 프로그래밍 추상화이다.</p>
<ul>
<li>Triton은 파라메트릭 타일 변수(parametric tile variables)를 사용하는 C 기반 언어(Triton-C)와 LLVM 기반의 중간 표현(Triton-IR)을 활용</li>
<li>텐서 프로그램을 효율적인 GPU 코드로 컴파일</li>
<li>결과적으로 cuBLAS와 대등한 성능을 내면서도 Shift convolution 같은 새로운 연구 아이디어를 효율적으로 구현</li>
</ul>
</li>
</ul>
<p>물론 장점만 있는건 아님.
관련 내용은 아래 링크로</p>
<p><a href="https://cpist.github.io/blog/posts/triton/tritonwithcuda/">Link</a></p></div><hr style="border:0;border-top:1px solid #eee;margin:36px 0"/><div></div></article><!--$--><!--/$--><footer style="margin-top:48px;padding-top:16px;border-top:1px solid #eee;color:#666">© <!-- -->2026<!-- --> CPIST&#x27;s blog</footer></main><script src="/blog/_next/static/chunks/2139e000f4b5d584.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[22016,[\"/blog/_next/static/chunks/796e69ae18b2784c.js\"],\"\"]\n3:I[39756,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n4:I[37457,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n6:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"OutletBoundary\"]\n7:\"$Sreact.suspense\"\n9:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"ViewportBoundary\"]\nb:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"MetadataBoundary\"]\nd:I[68027,[],\"default\"]\n:HL[\"/blog/_next/static/chunks/d335db6b3f1a2619.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"TI0pD73CnhalHln6RXV_z\",\"c\":[\"\",\"posts\",\"paper\",\"triton\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"paper/triton\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/blog/_next/static/chunks/d335db6b3f1a2619.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/blog/_next/static/chunks/796e69ae18b2784c.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"ko\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"main\",null,{\"children\":[[\"$\",\"header\",null,{\"style\":{\"display\":\"flex\",\"justifyContent\":\"space-between\",\"alignItems\":\"baseline\",\"gap\":12},\"children\":[[\"$\",\"h1\",null,{\"style\":{\"margin\":\"8px 0\"},\"children\":[\"$\",\"$L2\",null,{\"href\":\"/\",\"style\":{\"textDecoration\":\"none\"},\"children\":\"CPIST's blog\"}]}],[\"$\",\"nav\",null,{\"style\":{\"display\":\"flex\",\"gap\":12},\"children\":[\"$\",\"$L2\",null,{\"href\":\"/posts\",\"children\":\"Posts\"}]}]]}],[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"12px 0 24px\"}}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"style\":{\"marginTop\":48,\"paddingTop\":16,\"borderTop\":\"1px solid #eee\",\"color\":\"#666\"},\"children\":[\"© \",2026,\" CPIST's blog\"]}]]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/blog/_next/static/chunks/631eeae4923b8465.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L6\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@8\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L9\",null,{\"children\":\"$@a\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@c\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"article\",null,{\"children\":[[\"$\",\"div\",null,{\"style\":{\"color\":\"#666\",\"marginBottom\":6},\"children\":[\"$\",\"a\",null,{\"href\":\"/paper\",\"style\":{\"textDecoration\":\"none\"},\"children\":[\"/\",\"paper\"]}]}],[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0},\"children\":\"Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations\"}],[\"$\",\"div\",null,{\"style\":{\"color\":\"#666\",\"marginBottom\":18},\"children\":\"2026-01-04\"}],[\"$\",\"p\",null,{\"style\":{\"color\":\"#333\"},\"children\":\"Structure of Triton\"}],[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"18px 0\"}}],[\"$\",\"div\",null,{\"className\":\"prose\",\"children\":[[\"$\",\"p\",null,{\"children\":\"Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations (2019) 는 MAPL 워크숍에서 발표된 논문임.\\nTriton 의 구조에 대해 다루고 있음\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"1) Triton Architecture Overview\\nTriton의 아키텍처는 크게 3단계로 나뉜다.\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Triton-C (Frontend)\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Tile Semantics: ANSI C(CUDA-C) 문법을 따르지만, int tile[16, 16]과 같이 **Tile(다차원 배열)**을 일급 객체(First-class citizen)로 취급\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Broadcasting: NumPy와 유사한 브로드캐스팅 규칙을 언어 차원에서 지원하여 차원이 다른 텐서 간의 연산을 직관적으로 표현\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"SPMD on Blocks: CUDA와 달리 커널이 Single-threaded로 작성되지만\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"각 인스턴스가 자동으로 병렬화되어 Global Range(Grid)의 특정 타일을 처리\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"개발자는 스레드 간 동기화(Shared memory sync)를 직접 신경 쓸 필요가 없음\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Triton-IR (Middle-end)\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"LLVM-based: LLVM-IR과 구조가 유사하지만 타일 레벨의 데이터 흐름(Data-flow)과 제어 흐름(Control-flow) 분석을 위한 확장이 포함됨\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Tile Instructions: reshape, broadcast, dot, trans 같은 타일 전용 명령어가 추가됨\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Predication: 타일 내부의 개별 요소에 대한 분기 처리를 위해 Predicated SSA (PSSA) 형식을 사용하여 제어 흐름을 표현\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Triton-JIT (Backend)\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Triton-IR을 입력받아 최적화된 머신 코드(PTX/LLVM bitcode)를 생성\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Auto-tuner가 내장되어 있어 타일 크기 등의 메타 파라미터를 자동으로 최적화\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"2) Key Compiler Optimizations\\nTriton-JIT의 핵심 최적화 패스(Pass)들은 다음과 같음.\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Machine-Independent Passes\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Prefetching: 루프 내에서 타일 레벨 메모리 연산으로 인한 지연(Latency)을 숨기고자 함\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"컴파일러가 루프를 감지하고 적절한 프리패칭(Pre-fetching) 코드를 삽입\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Peephole Optimization: 타일 대수(Algebra) 속성을 이용해 Trans(Trans(X)) == X와 같은 패턴을 감지하고 단순화\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Machine-Dependent Passes (GPU Targeting)\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Hierarchical Tiling (계층적 타일링)\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"타일을 Micro-tile, Nano-tile로 분해하여 GPU의 메모리 계층 구조(DRAM \",[\"$\",\"code\",null,{\"children\":\"-\u003e\"}],\" L2 \",[\"$\",\"code\",null,{\"children\":\"-\u003e\"}],\" L1/Shared \",[\"$\",\"code\",null,{\"children\":\"-\u003e\"}],\" Register)에 딱 맞게 매핑\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Polyhedral 모델 없이도 중첩된 타일링 구성을 자동으로 열거하고 최적화 가능\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Memory Coalescing (메모리 결합)\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Triton-IR은 논리적으로 Single-threaded이지만\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"백엔드에서 이를 병렬화할 때 인접한 스레드가 인접한 메모리 주소에 접근하도록 스레드 순서를 재배치\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"이를 통해 DRAM 트랜잭션 효율을 극대화\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Shared Memory Allocation (공유 메모리 할당)\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"dot 연산처럼 연산 강도(Arithmetic Intensity)가 높은 작업의 피연산자를 Shared Memory에 저장\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"변수의 Live Range를 분석하여 선형 시간(Linear-time) 스토리지 할당 알고리즘을 사용해 최적의 위치를 결정\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Automatic Barrier Insertion (자동 동기화)\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"RAW(Read-After-Write) 및 WAR(Write-After-Read) 해저드를 감지하기 위해 데이터 흐름 분석(Data-flow analysis)을 수행\"}],\"\\n\",\"$Le\",\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\"]}],\"$L18\",\"$L19\"]}]\n"])</script><script>self.__next_f.push([1,"1a:I[80852,[\"/blog/_next/static/chunks/796e69ae18b2784c.js\",\"/blog/_next/static/chunks/631eeae4923b8465.js\"],\"default\"]\ne:[\"$\",\"li\",null,{\"children\":\"필요한 지점에 자동으로 Barrier를 삽입하여 프로그램의 정확성을 보장\"}]\nf:[\"$\",\"p\",null,{\"children\":\"3) Performance Benchmarks\"}]\n10:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Matrix Multiplication: DeepSpeech2, Transformer 등의 워크로드에서 cuBLAS와 대등한 성능을 보임\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"또한 타 DSL(Halide, TVM, PlaidML 등) 대비 최대 2-3배 빠른 성능을 기록\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Convolutions: cuDNN의 IMPLICIT_GEMM 알고리즘을 재구현했을 때 성능 저하가 없음\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"특히 ResNet 일부 태스크에서는 cuDNN보다 뛰어난 성능을 보임\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Shift-Convolutions: 기존에는 핸드 튜닝된 커널과 cuBLAS 호출을 섞어 써야 했던 Shift-Conv를 Triton-C로 하나의 융합된(Fused) 커널로 구현\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Shift 연산 비용을 거의 완벽하게 숨기는 성능을 달성\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n11:[\"$\",\"p\",null,{\"children\":\"4) 엔지니어로서의 포인트\"}]\n"])</script><script>self.__next_f.push([1,"12:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Tile Semantics \u0026 Broadcasting\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"기존의 스칼라 기반 루프 최적화(Loop transformation) 접근법 대신, Tile 자체를 타입으로 정의하고 연산 단위로 삼는 것이 하드웨어(Tensor Core 등) 활용에 얼마나 유리한지를 확인\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Broadcasting 규칙을 IR 레벨에서 지원함으로써 차원 불일치 문제를 우아하게 해결하고 벡터화(Vectorization) 기회를 명시적으로 드러냄\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"IR Design: Predicated SSA\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"GPU와 같은 SIMT 구조를 컴파일할 때 가장 까다로운 것 중 하나가 Divergent Control Flow\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Triton은 타일 내부의 제어 흐름을 분기(Branch)가 아닌 Predicated SSA 형식(psi 명령어 등)으로 처리\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"벡터화된 실행 흐름을 유지하면서도 조건부 연산을 효율적으로 표현\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"이는 마스킹(Masking)을 통한 벡터 연산 최적화의 핵심이자, 배울 부분\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Automatic Synchronization\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"CUDA 프로그래밍의 가장 큰 진입 장벽인 __syncthreads() 수동 관리를 컴파일러 패스로 해결\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"**데이터 흐름 분석(Liveness Analysis)**을 통해 공유 메모리 접근 간의 의존성을 파악하고 Barrier를 자동 삽입\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"이건 고성능 병렬 컴파일러의 핵심적으로 다루어야 할 부분이라 할 수 있음\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Optimization Space Extraction\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Auto-tuner가 단순히 미리 정의된 템플릿을 튜닝하는 것이 아니라, IR에서 직접 최적화 공간(타일 사이즈, 계층 구조 등)을 추출하여 탐색\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"컴파일러 기반 오토튜닝의 이상적인 방향성을 보여줬다고도 할 수 있다.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"13:[\"$\",\"p\",null,{\"children\":\"===============================================\"}]\n14:[\"$\",\"p\",null,{\"children\":\"이 논문을 요약해 보자면 왜 Triton을 구현했고, 어떻게 구현했는가에 포커싱 되어 있음.\"}]\n15:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"딥러닝 분야에서 새로운 아이디어를 검증하고 배포할 때, 효율적인 연산 커널(Compute Kernel)의 부재가 병목이 되는 경우가 많음\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"cuBLAS나 cuDNN 같은 벤더 라이브러리는 성능이 뛰어나지만 제한된 연산만 지원하며\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"TVM이나 PlaidML 같은 DSL은 유연하지만 벤더 라이브러리만큼의 성능을 내기 어려움\\nTriton은 이러한 문제를 해결하기 위해 제안된 언어이자 컴파일러이다.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"여기서 핵심 아이디어는 \\\"Tile(타일)\\\" 개념을 중심으로 한 프로그래밍 추상화이다.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Triton은 파라메트릭 타일 변수(parametric tile variables)를 사용하는 C 기반 언어(Triton-C)와 LLVM 기반의 중간 표현(Triton-IR)을 활용\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"텐서 프로그램을 효율적인 GPU 코드로 컴파일\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"결과적으로 cuBLAS와 대등한 성능을 내면서도 Shift convolution 같은 새로운 연구 아이디어를 효율적으로 구현\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n16:[\"$\",\"p\",null,{\"children\":\"물론 장점만 있는건 아님.\\n관련 내용은 아래 링크로\"}]\n17:[\"$\",\"p\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://cpist.github.io/blog/posts/triton/tritonwithcuda/\",\"children\":\"Link\"}]}]\n18:[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"36px 0\"}}]\n19:[\"$\",\"$L1a\",null,{}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"1b:I[27201,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"IconMark\"]\nc:[[\"$\",\"title\",\"0\",{\"children\":\"Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations - CPIST's blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Structure of Triton\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/blog/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L1b\",\"3\",{}]]\n8:null\n"])</script></body></html>