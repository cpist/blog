<!DOCTYPE html><!--LsmVKRpGgXNWls7UQ5kSE--><html lang="ko"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/blog/_next/static/chunks/d335db6b3f1a2619.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/blog/_next/static/chunks/cbd55ab9639e1e66.js"/><script src="/blog/_next/static/chunks/8c4bb65ca9f95eb5.js" async=""></script><script src="/blog/_next/static/chunks/0ff423a9fcc0186e.js" async=""></script><script src="/blog/_next/static/chunks/88a8688d62cd2814.js" async=""></script><script src="/blog/_next/static/chunks/turbopack-8ff2f6efb1e78309.js" async=""></script><script src="/blog/_next/static/chunks/796e69ae18b2784c.js" async=""></script><script src="/blog/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/blog/_next/static/chunks/247eb132b7f7b574.js" async=""></script><title>MLIR Transform Tutorial Ch 0 에 대하여 - CPIST&#x27;s blog</title><meta name="description" content="A Primer on “Structured” Linalg Operations"/><link rel="icon" href="/blog/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/blog/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><main><header style="display:flex;justify-content:space-between;align-items:baseline;gap:12px"><h1 style="margin:8px 0"><a style="text-decoration:none" href="/blog/">CPIST&#x27;s blog</a></h1><nav style="display:flex;gap:12px"><a href="/blog/posts/">Posts</a></nav></header><hr style="border:0;border-top:1px solid #eee;margin:12px 0 24px"/><article><div style="color:#666;margin-bottom:6px"><a href="/mlir" style="text-decoration:none">/<!-- -->mlir</a></div><h2 style="margin-top:0">MLIR Transform Tutorial Ch 0 에 대하여</h2><div style="color:#666;margin-bottom:18px">2025-11-16</div><p style="color:#333">A Primer on “Structured” Linalg Operations</p><hr style="border:0;border-top:1px solid #eee;margin:18px 0"/><div class="prose"><ol>
<li>
<p>구조화된 (Structured) Linalg 연산에 대해 다루어 본다.
기본적으로 MLIR의 Linalg는 &quot;계산의 모양 (구조)&quot; 을 오래 유지해서, 컴파일러가 똑똑하게 최적화(벡터화, 타일링, 퓨전 등) 할 수 있게 해주는 방식이다.</p>
</li>
<li>
<p>한번에 여러개 계산하기 (Uniform Elementwise)</p>
</li>
</ol>
<pre><code>%2 = arith.addf %0, %1 : f32
</code></pre>
<p>이런건 숫자 하나 + 숫자 하나인데</p>
<pre><code>%2 = arith.addf %0, %1 : vector&lt;8xf32&gt;
%2 = arith.addf %0, %1 : vector&lt;8x4xf32&gt;
</code></pre>
<p>MLIR에서는 위와 같이도 쓴다.</p>
<ul>
<li>모든 원소에 똑같은 연산이라는 구조를 유지한다.</li>
<li>컴파일러가<!-- -->
<ul>
<li>벡터 명령으로 바꾸거나</li>
<li>차원을 쪼개거나</li>
<li>add + mul 을 fused instruction 으로 합치기가 쉬워진다.</li>
</ul>
</li>
</ul>
<ol start="3">
<li>벡터를 하나로 줄이기 (Reduction)</li>
</ol>
<pre><code>// 벡터 전체를 더해서 스칼라 하나 만들기
%1 = vector.reduction &lt;add&gt;, %0 : vector&lt;8xf32&gt; into f32
</code></pre>
<p>이건 사실 아래와 같다.</p>
<pre><code>sum = 0
for i in 0..7:
  sum += v[i]
</code></pre>
<p>왜 굳이 이렇게 하는가?</p>
<ul>
<li>어떤 하드웨어는 reduce 전용 명령어가 있다.</li>
<li>어떤 경우엔 루프를 풀어서(unroll) 하는 게 더 빠를 수도 있음.
이건 reduction 이라고 말해주면, 구현은 컴파일러가 선택함.</li>
</ul>
<ol start="4">
<li>곱하고 더하기 = contraction (Dot / Matmul 의 일반형)
Dot product 도 사실 이런 형태이다.</li>
</ol>
<pre><code>sum += a[i] * b[i]
</code></pre>
<p>MLIR에서는 아래와 같이 씀</p>
<pre><code>vector.contract { ... } %a, %b, %init
</code></pre>
<p>여기서 중요한 개념이 2가지가 있음.</p>
<ol>
<li>indexing_maps</li>
</ol>
<ul>
<li>이 차원에서 어떤 인덱스를 쓰는지</li>
</ul>
<ol start="2">
<li>iterator_types</li>
</ol>
<ul>
<li>&quot;parallel&quot; -&gt; 그대로 유지</li>
<li>&quot;reduction&quot; -&gt; 줄이는 차원</li>
</ul>
<p>그 결과</p>
<ul>
<li>Dot product</li>
<li>Matrix x Matrix</li>
<li>Batch Matmul
전부 같은 틀로 표현이 가능하게 된다.
이건 matmul 이라는 것을 컴파일러가 바로 알아볼 수 있음</li>
</ul>
<ol start="5">
<li>벡터 -&gt; 메모리로 확장 (linalg.generic)
레지스터 (vector) 말고 메모리 (memref / tensor) 에서 계산해보기</li>
</ol>
<pre><code>linalg.generic {
  indexing_maps = ...
  iterator_types = ...
} ins(...) outs(...) {
  %x = mul
  %y = add
  linalg.yield %y
}
</code></pre>
<p>내용은 생각보다 단순하다.</p>
<ul>
<li>ins : 읽기 전용 입력</li>
<li>outs : 누적해서 쓰는 출력</li>
<li>region : 원소 하나에 대해 뭘 할지 적어둔 레시피
for 문 없이 for 문 의미를 표현함</li>
</ul>
<ol start="6">
<li>암묵적인 Loop Fusion (임시 버퍼 제거)</li>
</ol>
<pre><code>relu(x) = max(0, x)
</code></pre>
<p>보통은 비교 -&gt; 선택 -&gt; 임시버퍼인데
이를 linalg.generic 안에 다 넣으면
cmp -&gt; select -&gt; yield 가 된다.</p>
<ul>
<li>임시 버퍼가 없고</li>
<li>루프는 딱 한번이고</li>
<li>나중에 벡터화 / 루프화 자유가 된다.</li>
</ul>
<p>루프 안에서 할 일을 한번에 할 수 있다.</p>
<ol start="7">
<li>Tensor 버전 : 더 똑똑한 추상화
Tensor는</li>
</ol>
<ul>
<li>읽기 전용</li>
<li>결과는 새 tensor로 생성</li>
<li>alias 걱정이 없다.</li>
</ul>
<pre><code>%result = linalg.generic ... -&gt; tensor&lt;...&gt;
</code></pre>
<p>장점은</p>
<ul>
<li>의존성 분석이 쉽고</li>
<li>병렬화, 재배치, 퓨전에 최적화 되어 있음.</li>
<li>컴파일러가 제일 좋아하는 형태이다.</li>
</ul>
<ol start="8">
<li>타일링 = 암묵적 루프를 실제 루프로
타일링이란?
큰 계산을 여러개로</li>
</ol>
<pre><code>8x16 → (2x8) 타일 여러 개
</code></pre>
<p>MLIR 에서는</p>
<ul>
<li>같은 linalg.generic</li>
<li>slice 만 바꿔서</li>
<li>바깥에 scf.forall 루프를 생성한다.</li>
<li>즉 계산은 그대로인데 범위만 쪼개는 것임.</li>
</ul>
<ol start="9">
<li>Producer / Consumer Fusion (Rematerialization)
예를들어 matmul 결과 <code>-&gt;</code> elementwise 연산 일때</li>
</ol>
<ul>
<li>이러면 전채 결과를 메모리에 쓸 필요가 없다.</li>
</ul>
<p>따라서</p>
<ul>
<li>타일 단위로<!-- -->
<ul>
<li>matmul 부분 계산</li>
<li>바로 elementwise 연산</li>
<li>결과만 저장한다.</li>
</ul>
</li>
<li>메모리 트래픽이 감소되고</li>
<li>계산 조금 중보 될수도 있다. (rematerialization)
메모리 vs 계산량 트레이드 오프를 컴파일러가 선택한다.</li>
</ul>
<ol start="10">
<li>귀찮을 때는 이름 있는 연산을 쓰기</li>
</ol>
<pre><code>linalg.matmul ...

위는 사실

linalg.generic { indexing_maps = ..., iterator_types = ... }
의 축약형이다.
</code></pre>
<p>읽기도 쉽고 의미도 명확하다.</p>
<p>=====================================================================</p>
<p>요약해 보자면 Structured Linalg는 의미를 오래 들고 있다가 하드웨어에 맞게 코드를 생성하는 방식이다.
의미를 남겨두면 컴파일러가 최적화 할수 있게 한다.</p></div></article><!--$--><!--/$--><footer style="margin-top:48px;padding-top:16px;border-top:1px solid #eee;color:#666">© <!-- -->2026<!-- --> CPIST&#x27;s blog</footer></main><script src="/blog/_next/static/chunks/cbd55ab9639e1e66.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[22016,[\"/blog/_next/static/chunks/796e69ae18b2784c.js\"],\"\"]\n3:I[39756,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n4:I[37457,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n6:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"OutletBoundary\"]\n7:\"$Sreact.suspense\"\n9:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"ViewportBoundary\"]\nb:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"MetadataBoundary\"]\nd:I[68027,[],\"default\"]\n:HL[\"/blog/_next/static/chunks/d335db6b3f1a2619.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"LsmVKRpGgXNWls7UQ5kSE\",\"c\":[\"\",\"posts\",\"mlir\",\"md-t0\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"mlir/md-t0\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/blog/_next/static/chunks/d335db6b3f1a2619.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/blog/_next/static/chunks/796e69ae18b2784c.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"ko\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"main\",null,{\"children\":[[\"$\",\"header\",null,{\"style\":{\"display\":\"flex\",\"justifyContent\":\"space-between\",\"alignItems\":\"baseline\",\"gap\":12},\"children\":[[\"$\",\"h1\",null,{\"style\":{\"margin\":\"8px 0\"},\"children\":[\"$\",\"$L2\",null,{\"href\":\"/\",\"style\":{\"textDecoration\":\"none\"},\"children\":\"CPIST's blog\"}]}],[\"$\",\"nav\",null,{\"style\":{\"display\":\"flex\",\"gap\":12},\"children\":[\"$\",\"$L2\",null,{\"href\":\"/posts\",\"children\":\"Posts\"}]}]]}],[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"12px 0 24px\"}}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"style\":{\"marginTop\":48,\"paddingTop\":16,\"borderTop\":\"1px solid #eee\",\"color\":\"#666\"},\"children\":[\"© \",2026,\" CPIST's blog\"]}]]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",null,[\"$\",\"$L6\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@8\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L9\",null,{\"children\":\"$@a\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@c\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"article\",null,{\"children\":[[\"$\",\"div\",null,{\"style\":{\"color\":\"#666\",\"marginBottom\":6},\"children\":[\"$\",\"a\",null,{\"href\":\"/mlir\",\"style\":{\"textDecoration\":\"none\"},\"children\":[\"/\",\"mlir\"]}]}],[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0},\"children\":\"MLIR Transform Tutorial Ch 0 에 대하여\"}],[\"$\",\"div\",null,{\"style\":{\"color\":\"#666\",\"marginBottom\":18},\"children\":\"2025-11-16\"}],[\"$\",\"p\",null,{\"style\":{\"color\":\"#333\"},\"children\":\"A Primer on “Structured” Linalg Operations\"}],[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"18px 0\"}}],[\"$\",\"div\",null,{\"className\":\"prose\",\"children\":[[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"구조화된 (Structured) Linalg 연산에 대해 다루어 본다.\\n기본적으로 MLIR의 Linalg는 \\\"계산의 모양 (구조)\\\" 을 오래 유지해서, 컴파일러가 똑똑하게 최적화(벡터화, 타일링, 퓨전 등) 할 수 있게 해주는 방식이다.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"한번에 여러개 계산하기 (Uniform Elementwise)\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"%2 = arith.addf %0, %1 : f32\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이런건 숫자 하나 + 숫자 하나인데\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"%2 = arith.addf %0, %1 : vector\u003c8xf32\u003e\\n%2 = arith.addf %0, %1 : vector\u003c8x4xf32\u003e\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"MLIR에서는 위와 같이도 쓴다.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"모든 원소에 똑같은 연산이라는 구조를 유지한다.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"컴파일러가\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"벡터 명령으로 바꾸거나\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"차원을 쪼개거나\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"add + mul 을 fused instruction 으로 합치기가 쉬워진다.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"ol\",null,{\"start\":\"3\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"벡터를 하나로 줄이기 (Reduction)\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"// 벡터 전체를 더해서 스칼라 하나 만들기\\n%1 = vector.reduction \u003cadd\u003e, %0 : vector\u003c8xf32\u003e into f32\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이건 사실 아래와 같다.\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"sum = 0\\nfor i in 0..7:\\n  sum += v[i]\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"왜 굳이 이렇게 하는가?\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"어떤 하드웨어는 reduce 전용 명령어가 있다.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"어떤 경우엔 루프를 풀어서(unroll) 하는 게 더 빠를 수도 있음.\\n이건 reduction 이라고 말해주면, 구현은 컴파일러가 선택함.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ol\",null,{\"start\":\"4\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"곱하고 더하기 = contraction (Dot / Matmul 의 일반형)\\nDot product 도 사실 이런 형태이다.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"sum += a[i] * b[i]\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"MLIR에서는 아래와 같이 씀\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"vector.contract { ... } %a, %b, %init\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"여기서 중요한 개념이 2가지가 있음.\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"indexing_maps\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"이 차원에서 어떤 인덱스를 쓰는지\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ol\",null,{\"start\":\"2\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"iterator_types\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"\\\"parallel\\\" -\u003e 그대로 유지\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"\\\"reduction\\\" -\u003e 줄이는 차원\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"그 결과\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Dot product\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Matrix x Matrix\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Batch Matmul\\n전부 같은 틀로 표현이 가능하게 된다.\\n이건 matmul 이라는 것을 컴파일러가 바로 알아볼 수 있음\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ol\",null,{\"start\":\"5\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"벡터 -\u003e 메모리로 확장 (linalg.generic)\\n레지스터 (vector) 말고 메모리 (memref / tensor) 에서 계산해보기\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"linalg.generic {\\n  indexing_maps = ...\\n  iterator_types = ...\\n} ins(...) outs(...) {\\n  %x = mul\\n  %y = add\\n  linalg.yield %y\\n}\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"내용은 생각보다 단순하다.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"ins : 읽기 전용 입력\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"outs : 누적해서 쓰는 출력\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"region : 원소 하나에 대해 뭘 할지 적어둔 레시피\\nfor 문 없이 for 문 의미를 표현함\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ol\",null,{\"start\":\"6\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"암묵적인 Loop Fusion (임시 버퍼 제거)\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"relu(x) = max(0, x)\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"보통은 비교 -\u003e 선택 -\u003e 임시버퍼인데\\n이를 linalg.generic 안에 다 넣으면\\ncmp -\u003e select -\u003e yield 가 된다.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"임시 버퍼가 없고\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"루프는 딱 한번이고\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"나중에 벡터화 / 루프화 자유가 된다.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"루프 안에서 할 일을 한번에 할 수 있다.\"}],\"\\n\",[\"$\",\"ol\",null,{\"start\":\"7\",\"children\":[\"\\n\",\"$Le\",\"\\n\"]}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\"]}]]}]\n"])</script><script>self.__next_f.push([1,"e:[\"$\",\"li\",null,{\"children\":\"Tensor 버전 : 더 똑똑한 추상화\\nTensor는\"}]\nf:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"읽기 전용\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"결과는 새 tensor로 생성\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"alias 걱정이 없다.\"}],\"\\n\"]}]\n10:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"%result = linalg.generic ... -\u003e tensor\u003c...\u003e\\n\"}]}]\n11:[\"$\",\"p\",null,{\"children\":\"장점은\"}]\n12:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"의존성 분석이 쉽고\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"병렬화, 재배치, 퓨전에 최적화 되어 있음.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"컴파일러가 제일 좋아하는 형태이다.\"}],\"\\n\"]}]\n13:[\"$\",\"ol\",null,{\"start\":\"8\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"타일링 = 암묵적 루프를 실제 루프로\\n타일링이란?\\n큰 계산을 여러개로\"}],\"\\n\"]}]\n14:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"8x16 → (2x8) 타일 여러 개\\n\"}]}]\n15:[\"$\",\"p\",null,{\"children\":\"MLIR 에서는\"}]\n16:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"같은 linalg.generic\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"slice 만 바꿔서\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"바깥에 scf.forall 루프를 생성한다.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"즉 계산은 그대로인데 범위만 쪼개는 것임.\"}],\"\\n\"]}]\n17:[\"$\",\"ol\",null,{\"start\":\"9\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Producer / Consumer Fusion (Rematerialization)\\n예를들어 matmul 결과 \",[\"$\",\"code\",null,{\"children\":\"-\u003e\"}],\" elementwise 연산 일때\"]}],\"\\n\"]}]\n18:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"이러면 전채 결과를 메모리에 쓸 필요가 없다.\"}],\"\\n\"]}]\n19:[\"$\",\"p\",null,{\"children\":\"따라서\"}]\n1a:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"타일 단위로\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"matmul 부분 계산\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"바로 elementwise 연산\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"결과만 저장한다.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"메모리 트래픽이 감소되고\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"계산 조금 중보 될수도 있다. (rematerialization)\\n메모리 vs 계산량 트레이드 오프를 컴파일러가 선택한다.\"}],\"\\n\"]}]\n1b:[\"$\",\"ol\",null,{\"start\":\"10\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"귀찮을 때는 이름 있는 연산을 쓰기\"}],\"\\n\"]}]\n1c:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"linalg.matmul ...\\n\\n위는 사실\\n\\nlinalg.generic { indexing_maps = ..., iterator_types = ... }\\n의 축약형이다.\\n\"}]}]\n1d:[\"$\",\"p\",null,{\"children\":\"읽기도 쉽고 의미도 명확하다.\"}]\n1e:[\"$\",\"p\",null,{\"children\":\"=====================================================================\"}]\n1f:[\"$\",\"p\",null,{\"children\":\"요약해 보자면 Structured Linalg는 의미를 오래 들고 있다가 하드웨어에 맞게 코드를 생성하는 방식이다.\\n의미를 남겨두면 컴파일러가 최적화 할수 있게 한다.\"}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"20:I[27201,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"IconMark\"]\nc:[[\"$\",\"title\",\"0\",{\"children\":\"MLIR Transform Tutorial Ch 0 에 대하여 - CPIST's blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"A Primer on “Structured” Linalg Operations\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/blog/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L20\",\"3\",{}]]\n8:null\n"])</script></body></html>