<!DOCTYPE html><!--Vbrn69KQA2pbTlqg8boA0--><html lang="ko"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/blog/_next/static/chunks/d335db6b3f1a2619.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/blog/_next/static/chunks/2139e000f4b5d584.js"/><script src="/blog/_next/static/chunks/8a8ef77865bda9e6.js" async=""></script><script src="/blog/_next/static/chunks/0ff423a9fcc0186e.js" async=""></script><script src="/blog/_next/static/chunks/88a8688d62cd2814.js" async=""></script><script src="/blog/_next/static/chunks/turbopack-566b9f8f22ac84c4.js" async=""></script><script src="/blog/_next/static/chunks/796e69ae18b2784c.js" async=""></script><script src="/blog/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/blog/_next/static/chunks/247eb132b7f7b574.js" async=""></script><script src="/blog/_next/static/chunks/631eeae4923b8465.js" async=""></script><link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" as="style" crossorigin="anonymous" integrity="sha384-n8MVd4RsNIU0KOVEMVIqhKyMVPsoloXttrTHYUjDkaWaXIhKbMCh2GbqNl2CAPFu"/><title>MLIR Toy Tutorial Ch 3 에 대하여 - CPIST&#x27;s blog</title><meta name="description" content="High-level language-specific optimization using pattern rewriting system."/><link rel="icon" href="/blog/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0KOVEMVIqhKyMVPsoloXttrTHYUjDkaWaXIhKbMCh2GbqNl2CAPFu" crossorigin="anonymous"/><script src="/blog/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><main><header style="display:flex;justify-content:space-between;align-items:baseline;gap:12px"><h1 style="margin:8px 0"><a style="text-decoration:none" href="/blog/">CPIST&#x27;s blog</a></h1><nav style="display:flex;gap:12px"><a href="/blog/posts/">Posts</a></nav></header><hr style="border:0;border-top:1px solid #eee;margin:12px 0 24px"/><article><div style="color:#666;margin-bottom:6px"><a href="/mlir" style="text-decoration:none">/<!-- -->mlir</a></div><h2 style="margin-top:0">MLIR Toy Tutorial Ch 3 에 대하여</h2><div style="color:#666;margin-bottom:18px">2025-11-01</div><p style="color:#333">High-level language-specific optimization using pattern rewriting system.</p><hr style="border:0;border-top:1px solid #eee;margin:18px 0"/><div class="prose"><p>1) 왜 고수준 dialect인가?
LLVM IR로 내려가면 많은 게 루프와 메모리 load / store로 풀림.</p>
<ul>
<li>당연히 이런 구조면 transpose(transpose(x)) 같은 걸 알아 보기 어렵고</li>
<li>임시 배열, 중간 버퍼가 끼면 더더욱 힘들어짐.
반면 MLIR의 Toy IR은 toy.transpose 같은 의미 있는 op로 남아 있어서</li>
<li>연산 의미를 그대로 보고</li>
<li>간단한 규칙으로 최적화 할 수 있다.</li>
</ul>
<p>2) 로컬 최적화는 패턴 매칭과 치환에 있다.
컴파일러 변환은 크게 두가지로 나뉨</p>
<ul>
<li>Local : 가까운 몇 개 op만 보고 바꾸는 최적화</li>
<li>Global : 함수 전체 / 프로그램 전체를 보고 바꾸는 최적화</li>
</ul>
<p>이번에 다루는 것은 Local인데, &#x27;이런 모양이면 이렇게 바꾼다&#x27;라는 것을 계속 적용하는 방식이다.
MLIR에는 이를 위한 두가지의 엔진이 있다.</p>
<ul>
<li>Generic DAG Rewriter</li>
<li>대표 실행 패스가 Canonicalizer (greedy 한 반복 적용 + deadcode 정리)</li>
</ul>
<ul>
<li>여기서 패스(pass) 라는 개념이 중요하다.
pass는 IR을 한번 훑으면서 분석하거나 변환하는 독립적인 단계를 말함.
즉, IR을 입력으로 받아서 정보를 얻거나 IR을 바꾸고 다음 단계로 넘기는 작업 단위를 말함.</li>
</ul>
<ul>
<li>pass는 IR을 한번 지나간다(pass through)</li>
<li>따라서 한 pass는 한번의 IR 통과를 의미하고, 여러 pass 를 연결하면 pass pipeline 이 되는 셈.</li>
<li>패스의 이점은 아래와 같음<!-- -->
<ul>
<li>분리와 재사용 (한 pass는 하나의 책임만, 다른 컴파일러와 언어에서도 재사용 가능)</li>
<li>순서 조절 가능 (최적화 순서 바꾸기 쉽고 실험하기 좋다.)</li>
<li>디버깅 쉬움 (이 패스를 돌린 뒤 IR이 어떻게 변했는지 바로 확인이 가능)
패스에는 크게 2가지로 나뉨. 분석 패스 (Analysis Pass) 와 변환 패스 (Transformation Pass)</li>
</ul>
</li>
<li>Analysis Pass는 IR을 읽기만 하고, 정보만 계산하는것<!-- -->
<ul>
<li>따라서 IR 변경을 안하고</li>
<li>메타정보 (analysis result)를 생성하고</li>
<li>다른 패스가 참고용으로 사용함.</li>
</ul>
</li>
<li>Transformation Pass 는 IR을 실제로 바꾸는 것<!-- -->
<ul>
<li>불필요한 연산을 제거한다던지, 연산 순서를 변경 한다던지, 고수준 op -&gt; 저수준 op로 변환 (lowering) 한다던지, 최적화를 한다.</li>
<li>우리가 다루는 Canonicalizer가 대표적인 변환 패스다.</li>
</ul>
</li>
</ul>
<ul>
<li>여기서 Canonicalizer 패스도 알아볼 필요가 있음.</li>
</ul>
<ul>
<li>알려진 단순 규칙들을 IR 전체에 반복적으로 적용해서 깔끔하게 만듦.<!-- -->
<ul>
<li>패턴 매칭 -&gt; Rewrite</li>
<li>더 이상 바뀔 게 없을 때까지 반복</li>
<li>dead code 정리를 포함함.</li>
<li>그래서 대표 실행 패스라고 부름.</li>
</ul>
</li>
</ul>
<ul>
<li>패스는 적용 대상 (scope) 이 있음.<!-- -->
<ul>
<li>ModulePass -&gt; 모듈 전체</li>
<li>FunctionPass -&gt; 함수 하나</li>
<li>LoopPass -&gt; 루프 하나</li>
</ul>
</li>
</ul>
<pre><code>pm.addNestedPass&lt;toy::FuncOp&gt;(createCanonicalizerPass());
</code></pre>
<p>이건 모듈 안에 있는 각 toy.func 마다 canonicalizer를 돌리라는 의미임.</p>
<ul>
<li>근데 여기서 어떻게 모듈 단위 / 함수 단위를 알수 있는 걸까?</li>
<li>PassManager는 ModuleOp 기준으로 만들어진 매니저임.</li>
</ul>
<pre><code>mlir::PassManager pm(module-&gt;getName());
</code></pre>
<ul>
<li>MLIR에서 최상위 IR 컨테이너는 항상 ModuleOp 임.</li>
<li>PassManger 는 기본적으로 ModuleOp에서 실행된다.</li>
<li>이 시점에서 패스 파이프라인은 모듈 단위로 시작한다는 것이 정해져 있다는 것.</li>
</ul>
<p>다시</p>
<pre><code>pm.addNestedPass&lt;toy::FuncOp&gt;(createCanonicalizerPass());

는 대충 아래같은 의미라는 것
runOnModule(module) {
  for (toy::FuncOp f : module.getOps&lt;toy::FuncOp&gt;()) {
    runCanonicalizerOnFunc(f);
  }
}
</code></pre>
<p>를 보면, ModuleOp 안에 중첩된(nested) toy::FuncOp를 대상으로 이 패스를 실행하라는게 명확해짐.</p>
<ul>
<li>Nested는 최상위(Module) 가 아니라, 그 안쪽에 들어있는 op들을 말하고 (2단계 scope)</li>
<li><code>&lt;toy::FuncOp&gt;</code> 는 그 안쪽 op 중에서 타입이 toy::FuncOp 인것을 말한다.</li>
</ul>
<pre><code>ModuleOp
 ├─ toy.func @f1
 ├─ toy.func @f2
 └─ toy.func @f3

여기서 패스를 실행하면

for (auto func : module.getOps&lt;toy::FuncOp&gt;()) {
    run canonicalizer on func;
}
</code></pre>
<p>즉,</p>
<ul>
<li>모듈을 한번 순회하고</li>
<li>그 안에서 각 toy.func를 찾아서 각 함수에 대해 canonicalizer 를 실행하라는 것이다.</li>
<li>다시 한번 말하지만 PassManager는 ModuleOp 기준이고, CanonicalizerPass는 FuncOp 기준이다.<!-- -->
<ul>
<li>그렇기 때문에 <code>addNestedPass&lt;FuncOp&gt;</code> 로 연결하는 것이다.</li>
<li>모듈을 기준으로 파이프라인을 시작해서, 그 안에 있는 각 함수 단위로 canonicalizer를 실행한다.</li>
</ul>
</li>
</ul>
<ul>
<li>만약 모듈단위 패스로 실행하고 싶다면?</li>
</ul>
<pre><code>pm.addPass(createCanonicalizerPass());
</code></pre>
<ul>
<li>만약 함수 단위 패스 매니저를 따로 만든다면?</li>
</ul>
<pre><code>OpPassManager &amp;funcPM = pm.nest&lt;toy::FuncOp&gt;();
funcPM.addPass(createCanonicalizerPass());
</code></pre>
<p>길게 확인해 봤는데 다시 본문으로 돌아오면</p>
<p>3) 예제 1: transpose(transpose(x)) → x (C++ 패턴 리라이터)
Toy 코드는</p>
<pre><code>return transpose(transpose(x));
</code></pre>
<p>Toy IR은</p>
<pre><code>%0 = toy.transpose(%arg0\)
%1 = toy.transpose(%0\)
return %1
</code></pre>
<ul>
<li>이걸 C++로도 할수 있음 (OpRewritePattern)</li>
<li>여기서 %0는 아무도 안쓰는 거니까, 사용처(use)가 0개인 값을 만드는 셈. 이런 op를 죽은코드(dead) 라고 함.</li>
<li>Pure trait 라는 것이 있음. op는 부작용이 없고, 같은 입력이면 항상 같은 출력만 만든다는 것.<!-- -->
<ul>
<li>transpose가 대표적인 예인데, 출력 tensor를 계산해서 만들어 내지만</li>
<li>출력 외에 프린트 / 메모리 변경 / IO / 전역상태 변경 같은 부작용이 없음</li>
<li>입력이 같으면 결과도 항상 같다.</li>
</ul>
</li>
<li>결과 SSA 값이 안쓰이는, 즉 안쓰이는 경우라면 얘를 제거해도 프로그램 관찰 가능한 결과 (출력, 메모리, 리턴값) 가 바뀌지 않기 때문에 제거해도 의미가 변하지 않음.</li>
<li>그래서 지울 수 있음.</li>
<li>MLIR은 SSA 기반이기 때문에 참조하는 곳이 0개면 그냥 dead value이고, 만약 op가 Pure(부작용 없음) 인 경우 바로 삭제함.<!-- -->
<ul>
<li>이것이 바로 dead code elimination(DCE) 의 기본 원리임.
다시 Toy case로 돌아와서</li>
</ul>
</li>
<li>MLIR이 op에 부작용 (side effect) 가 있을지도 몰라 보수적인 판단으로 함부로 못지울 수가 있음.</li>
<li>그래서 아래와 같이 Pure trait을 붙이는 것임.</li>
</ul>
<pre><code>def TransposeOp : Toy_Op&lt;&quot;transpose&quot;, [Pure]&gt; {...}
</code></pre>
<ul>
<li>MLIR은 transpose 는 부작용이 없기에 -&gt; 안쓰이면 지워도 된다고 판단함.</li>
<li>이에 따라 최종 결과가 toy.return %arg0 로 깔끔하게 나옴.</li>
</ul>
<p>4) reshape 최적화는 DRR(Declarative Rewrite Rules) 로 더 깔끔하게
Reshape 최적화는 규칙이 많고 반복적이기 떄문에, C++ 보다는 MLIR의 경우 DRR 라는 룰 기반 시스템을 활용함.
우리가 md2에서 ODS로 op를 정의했을때 DRR을 쓸수 있었다.</p>
<ul>
<li>DRR Rule example 1: Reshape(Reshape(x)) -&gt; Reshape(x)</li>
</ul>
<pre><code>def ReshapeReshapeOptPattern : Pat&lt;(ReshapeOp(ReshapeOp $arg)),
                                   (ReshapeOp $arg)&gt;;
</code></pre>
<ul>
<li>
<p>sourcePattern: reshape(reshape(arg))</p>
</li>
<li>
<p>resultPattern: reshape(arg) 하나만 남김</p>
</li>
<li>
<p>DRR Rule example 2 : 진짜 불필요한 reshape는 제거하기
만약 입력 타입과 출려 타입이 똑같은 경우 reshape는 완전히 쓸모가 없.
따라서 아래와 같이 조건을 추가함.</p>
</li>
</ul>
<pre><code>TypesAreIdentical: $0.getType() == $1.getType()
ReshapeOp:$res $arg  =&gt;  replaceWithValue $arg
</code></pre>
<ul>
<li>
<p>reshpae 결과 타입이 입력 타입과 같은 경우</p>
</li>
<li>
<p>reshape op를 없애고 입력을 그대로 쓰게 한다.</p>
</li>
<li>
<p>DRR Rule example 3 : Constant reshape는 상수 자체를 바꿔치기 하는 것</p>
</li>
</ul>
<p>reshape(constant) 는</p>
<ul>
<li>constant 의 dense 데이터를 그 shape으로 미리 바꿔서</li>
<li>reshape op를 통째로 삭제 가능하게 함.</li>
</ul>
<pre><code>(ReshapeOp:$res (ConstantOp $arg))
  =&gt;
(ConstantOp (ReshapeConstant $arg, $res))
</code></pre>
<ul>
<li>이건 Runtime reshape 이 아니라 Compile time에 상수 모양을 바꿔서 끝내는 것이다.</li>
</ul>
<p>5) 실제 케이스: trivial_reshape</p>
<pre><code>%1 = toy.reshape(%0\) to tensor&lt;2x1xf64&gt;
%2 = toy.reshape(%1\) to tensor&lt;2x1xf64&gt;
%3 = toy.reshape(%2\) to tensor&lt;2x1xf64&gt;
</code></pre>
<p>초기 IR은 reshpae이 이렇게 3번 연달아 있었지만
DRR과 canonicalizer를 돌리게 되면</p>
<pre><code>%0 = toy.constant dense&lt;[[1.0],[2.0]]&gt; : tensor&lt;2x1xf64&gt;
toy.print %0
</code></pre>
<p>이런 식으로 reshape이 전부 사라지고, constant 모양만 깔끔하게 맞춰진다.</p>
<p>=====================================================================</p>
<p>이번에도 긴글을 정리해 보자면</p>
<ul>
<li>Toy IR 처럼 의미가 살아있는 고수준 op들에 대해, 패턴 매칭으로 &#x27;쓸데없는 연산&#x27;을 싹 걷어내는 것.<!-- -->
<ul>
<li>C++ 방식 : OpRewritePattern 으로 직접 구현하는 것</li>
<li>선언형 방식 : DRR(TableGen 룰)로 간단히 정의되었음</li>
<li>Canonicalizer가 greedy 하게 반복 적용 + Pure 면 dead op 제거까지</li>
</ul>
</li>
</ul></div><hr style="border:0;border-top:1px solid #eee;margin:36px 0"/><div></div></article><!--$--><!--/$--><footer style="margin-top:48px;padding-top:16px;border-top:1px solid #eee;color:#666">© <!-- -->2026<!-- --> CPIST&#x27;s blog</footer></main><script src="/blog/_next/static/chunks/2139e000f4b5d584.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[22016,[\"/blog/_next/static/chunks/796e69ae18b2784c.js\"],\"\"]\n3:I[39756,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n4:I[37457,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n6:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"OutletBoundary\"]\n7:\"$Sreact.suspense\"\n9:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"ViewportBoundary\"]\nb:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"MetadataBoundary\"]\nd:I[68027,[],\"default\"]\n:HL[\"/blog/_next/static/chunks/d335db6b3f1a2619.css\",\"style\"]\n:HL[\"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css\",\"style\",{\"crossOrigin\":\"anonymous\",\"integrity\":\"sha384-n8MVd4RsNIU0KOVEMVIqhKyMVPsoloXttrTHYUjDkaWaXIhKbMCh2GbqNl2CAPFu\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"Vbrn69KQA2pbTlqg8boA0\",\"c\":[\"\",\"posts\",\"mlir\",\"md3\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"mlir/md3\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/blog/_next/static/chunks/d335db6b3f1a2619.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/blog/_next/static/chunks/796e69ae18b2784c.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"ko\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css\",\"integrity\":\"sha384-n8MVd4RsNIU0KOVEMVIqhKyMVPsoloXttrTHYUjDkaWaXIhKbMCh2GbqNl2CAPFu\",\"crossOrigin\":\"anonymous\"}]}],[\"$\",\"body\",null,{\"children\":[\"$\",\"main\",null,{\"children\":[[\"$\",\"header\",null,{\"style\":{\"display\":\"flex\",\"justifyContent\":\"space-between\",\"alignItems\":\"baseline\",\"gap\":12},\"children\":[[\"$\",\"h1\",null,{\"style\":{\"margin\":\"8px 0\"},\"children\":[\"$\",\"$L2\",null,{\"href\":\"/\",\"style\":{\"textDecoration\":\"none\"},\"children\":\"CPIST's blog\"}]}],[\"$\",\"nav\",null,{\"style\":{\"display\":\"flex\",\"gap\":12},\"children\":[\"$\",\"$L2\",null,{\"href\":\"/posts\",\"children\":\"Posts\"}]}]]}],[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"12px 0 24px\"}}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"style\":{\"marginTop\":48,\"paddingTop\":16,\"borderTop\":\"1px solid #eee\",\"color\":\"#666\"},\"children\":[\"© \",2026,\" CPIST's blog\"]}]]}]}]]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/blog/_next/static/chunks/631eeae4923b8465.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L6\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@8\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L9\",null,{\"children\":\"$@a\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@c\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"article\",null,{\"children\":[[\"$\",\"div\",null,{\"style\":{\"color\":\"#666\",\"marginBottom\":6},\"children\":[\"$\",\"a\",null,{\"href\":\"/mlir\",\"style\":{\"textDecoration\":\"none\"},\"children\":[\"/\",\"mlir\"]}]}],[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0},\"children\":\"MLIR Toy Tutorial Ch 3 에 대하여\"}],[\"$\",\"div\",null,{\"style\":{\"color\":\"#666\",\"marginBottom\":18},\"children\":\"2025-11-01\"}],[\"$\",\"p\",null,{\"style\":{\"color\":\"#333\"},\"children\":\"High-level language-specific optimization using pattern rewriting system.\"}],[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"18px 0\"}}],[\"$\",\"div\",null,{\"className\":\"prose\",\"children\":[[\"$\",\"p\",null,{\"children\":\"1) 왜 고수준 dialect인가?\\nLLVM IR로 내려가면 많은 게 루프와 메모리 load / store로 풀림.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"당연히 이런 구조면 transpose(transpose(x)) 같은 걸 알아 보기 어렵고\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"임시 배열, 중간 버퍼가 끼면 더더욱 힘들어짐.\\n반면 MLIR의 Toy IR은 toy.transpose 같은 의미 있는 op로 남아 있어서\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"연산 의미를 그대로 보고\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"간단한 규칙으로 최적화 할 수 있다.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"2) 로컬 최적화는 패턴 매칭과 치환에 있다.\\n컴파일러 변환은 크게 두가지로 나뉨\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Local : 가까운 몇 개 op만 보고 바꾸는 최적화\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Global : 함수 전체 / 프로그램 전체를 보고 바꾸는 최적화\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이번에 다루는 것은 Local인데, '이런 모양이면 이렇게 바꾼다'라는 것을 계속 적용하는 방식이다.\\nMLIR에는 이를 위한 두가지의 엔진이 있다.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Generic DAG Rewriter\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"대표 실행 패스가 Canonicalizer (greedy 한 반복 적용 + deadcode 정리)\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"여기서 패스(pass) 라는 개념이 중요하다.\\npass는 IR을 한번 훑으면서 분석하거나 변환하는 독립적인 단계를 말함.\\n즉, IR을 입력으로 받아서 정보를 얻거나 IR을 바꾸고 다음 단계로 넘기는 작업 단위를 말함.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"pass는 IR을 한번 지나간다(pass through)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"따라서 한 pass는 한번의 IR 통과를 의미하고, 여러 pass 를 연결하면 pass pipeline 이 되는 셈.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"패스의 이점은 아래와 같음\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"분리와 재사용 (한 pass는 하나의 책임만, 다른 컴파일러와 언어에서도 재사용 가능)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"순서 조절 가능 (최적화 순서 바꾸기 쉽고 실험하기 좋다.)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"디버깅 쉬움 (이 패스를 돌린 뒤 IR이 어떻게 변했는지 바로 확인이 가능)\\n패스에는 크게 2가지로 나뉨. 분석 패스 (Analysis Pass) 와 변환 패스 (Transformation Pass)\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Analysis Pass는 IR을 읽기만 하고, 정보만 계산하는것\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"따라서 IR 변경을 안하고\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"메타정보 (analysis result)를 생성하고\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"다른 패스가 참고용으로 사용함.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Transformation Pass 는 IR을 실제로 바꾸는 것\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"불필요한 연산을 제거한다던지, 연산 순서를 변경 한다던지, 고수준 op -\u003e 저수준 op로 변환 (lowering) 한다던지, 최적화를 한다.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"우리가 다루는 Canonicalizer가 대표적인 변환 패스다.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"여기서 Canonicalizer 패스도 알아볼 필요가 있음.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"알려진 단순 규칙들을 IR 전체에 반복적으로 적용해서 깔끔하게 만듦.\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"패턴 매칭 -\u003e Rewrite\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"더 이상 바뀔 게 없을 때까지 반복\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"dead code 정리를 포함함.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"그래서 대표 실행 패스라고 부름.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"패스는 적용 대상 (scope) 이 있음.\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"ModulePass -\u003e 모듈 전체\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"FunctionPass -\u003e 함수 하나\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"LoopPass -\u003e 루프 하나\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"pm.addNestedPass\u003ctoy::FuncOp\u003e(createCanonicalizerPass());\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"이건 모듈 안에 있는 각 toy.func 마다 canonicalizer를 돌리라는 의미임.\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"근데 여기서 어떻게 모듈 단위 / 함수 단위를 알수 있는 걸까?\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"PassManager는 ModuleOp 기준으로 만들어진 매니저임.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"mlir::PassManager pm(module-\u003egetName());\\n\"}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"MLIR에서 최상위 IR 컨테이너는 항상 ModuleOp 임.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"PassManger 는 기본적으로 ModuleOp에서 실행된다.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"이 시점에서 패스 파이프라인은 모듈 단위로 시작한다는 것이 정해져 있다는 것.\"}],\"\\n\"]}],\"\\n\",\"$Le\",\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\"]}],\"$L33\",\"$L34\"]}]\n"])</script><script>self.__next_f.push([1,"35:I[80852,[\"/blog/_next/static/chunks/796e69ae18b2784c.js\",\"/blog/_next/static/chunks/631eeae4923b8465.js\"],\"default\"]\ne:[\"$\",\"p\",null,{\"children\":\"다시\"}]\nf:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"pm.addNestedPass\u003ctoy::FuncOp\u003e(createCanonicalizerPass());\\n\\n는 대충 아래같은 의미라는 것\\nrunOnModule(module) {\\n  for (toy::FuncOp f : module.getOps\u003ctoy::FuncOp\u003e()) {\\n    runCanonicalizerOnFunc(f);\\n  }\\n}\\n\"}]}]\n10:[\"$\",\"p\",null,{\"children\":\"를 보면, ModuleOp 안에 중첩된(nested) toy::FuncOp를 대상으로 이 패스를 실행하라는게 명확해짐.\"}]\n11:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Nested는 최상위(Module) 가 아니라, 그 안쪽에 들어있는 op들을 말하고 (2단계 scope)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"\u003ctoy::FuncOp\u003e\"}],\" 는 그 안쪽 op 중에서 타입이 toy::FuncOp 인것을 말한다.\"]}],\"\\n\"]}]\n12:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"ModuleOp\\n ├─ toy.func @f1\\n ├─ toy.func @f2\\n └─ toy.func @f3\\n\\n여기서 패스를 실행하면\\n\\nfor (auto func : module.getOps\u003ctoy::FuncOp\u003e()) {\\n    run canonicalizer on func;\\n}\\n\"}]}]\n13:[\"$\",\"p\",null,{\"children\":\"즉,\"}]\n14:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"모듈을 한번 순회하고\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"그 안에서 각 toy.func를 찾아서 각 함수에 대해 canonicalizer 를 실행하라는 것이다.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"다시 한번 말하지만 PassManager는 ModuleOp 기준이고, CanonicalizerPass는 FuncOp 기준이다.\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"그렇기 때문에 \",[\"$\",\"code\",null,{\"children\":\"addNestedPass\u003cFuncOp\u003e\"}],\" 로 연결하는 것이다.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"모듈을 기준으로 파이프라인을 시작해서, 그 안에 있는 각 함수 단위로 canonicalizer를 실행한다.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n15:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"만약 모듈단위 패스로 실행하고 싶다면?\"}],\"\\n\"]}]\n16:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"pm.addPass(createCanonicalizerPass());\\n\"}]}]\n17:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"만약 함수 단위 패스 매니저를 따로 만든다면?\"}],\"\\n\"]}]\n18:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"OpPassManager \u0026funcPM = pm.nest\u003ctoy::FuncOp\u003e();\\nfuncPM.addPass(createCanonicalizerPass());\\n\"}]}]\n19:[\"$\",\"p\",null,{\"children\":\"길게 확인해 봤는데 다시 본문으로 돌아오면\"}]\n1a:[\"$\",\"p\",null,{\"children\":\"3) 예제 1: transpose(transpose(x)) → x (C++ 패턴 리라이터)\\nToy 코드는\"}]\n1b:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"return transpose(transpose(x));\\n\"}]}]\n1c:[\"$\",\"p\",null,{\"children\":\"Toy IR은\"}]\n1d:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"%0 = toy.transpose(%arg0\\\\)\\n%1 = toy.transpose(%0\\\\)\\nreturn %1\\n\"}]}]\n1e:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"이걸 C++로도 할수 있음 (OpRewritePattern)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"여기서 %0는 아무도 안쓰는 거니까, 사용처(use)가 0개인 값을 만드는 셈. 이런 op를 죽은코드(dead) 라고 함.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Pure trait 라는 것이 있음. op는 부작용이 없고, 같은 입력이면 항상 같은 출력만 만든다는 것.\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"transpose가 대표적인 예인데, 출력 tensor를 계산해서 만들어 내지만\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"출력 외에 프린트 / 메모리 변경 / IO / 전역상태 변경 같은 부작용이 없음\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"입력이 같으면 결과도 항상 같다.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"결과 SSA 값이 안쓰이는, 즉 안쓰이는 경우라면 얘를 제거해도 프로그램 관찰 가능한 결과 (출력, 메모리, 리턴값) 가 바뀌지 않기 때문에 제거해도 의미가 변하지 않음.\""])</script><script>self.__next_f.push([1,"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"그래서 지울 수 있음.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"MLIR은 SSA 기반이기 때문에 참조하는 곳이 0개면 그냥 dead value이고, 만약 op가 Pure(부작용 없음) 인 경우 바로 삭제함.\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"이것이 바로 dead code elimination(DCE) 의 기본 원리임.\\n다시 Toy case로 돌아와서\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"MLIR이 op에 부작용 (side effect) 가 있을지도 몰라 보수적인 판단으로 함부로 못지울 수가 있음.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"그래서 아래와 같이 Pure trait을 붙이는 것임.\"}],\"\\n\"]}]\n1f:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"def TransposeOp : Toy_Op\u003c\\\"transpose\\\", [Pure]\u003e {...}\\n\"}]}]\n20:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"MLIR은 transpose 는 부작용이 없기에 -\u003e 안쓰이면 지워도 된다고 판단함.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"이에 따라 최종 결과가 toy.return %arg0 로 깔끔하게 나옴.\"}],\"\\n\"]}]\n21:[\"$\",\"p\",null,{\"children\":\"4) reshape 최적화는 DRR(Declarative Rewrite Rules) 로 더 깔끔하게\\nReshape 최적화는 규칙이 많고 반복적이기 떄문에, C++ 보다는 MLIR의 경우 DRR 라는 룰 기반 시스템을 활용함.\\n우리가 md2에서 ODS로 op를 정의했을때 DRR을 쓸수 있었다.\"}]\n22:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"DRR Rule example 1: Reshape(Reshape(x)) -\u003e Reshape(x)\"}],\"\\n\"]}]\n23:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"def ReshapeReshapeOptPattern : Pat\u003c(ReshapeOp(ReshapeOp $arg)),\\n                                   (ReshapeOp $arg)\u003e;\\n\"}]}]\n24:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"sourcePattern: reshape(reshape(arg))\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"resultPattern: reshape(arg) 하나만 남김\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"DRR Rule example 2 : 진짜 불필요한 reshape는 제거하기\\n만약 입력 타입과 출려 타입이 똑같은 경우 reshape는 완전히 쓸모가 없.\\n따라서 아래와 같이 조건을 추가함.\"}],\"\\n\"]}],\"\\n\"]}]\n25:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"TypesAreIdentical: $0.getType() == $1.getType()\\nReshapeOp:$res $arg  =\u003e  replaceWithValue $arg\\n\"}]}]\n26:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"reshpae 결과 타입이 입력 타입과 같은 경우\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"reshape op를 없애고 입력을 그대로 쓰게 한다.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"DRR Rule example 3 : Constant reshape는 상수 자체를 바꿔치기 하는 것\"}],\"\\n\"]}],\"\\n\"]}]\n27:[\"$\",\"p\",null,{\"children\":\"reshape(constant) 는\"}]\n28:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"constant 의 dense 데이터를 그 shape으로 미리 바꿔서\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"reshape op를 통째로 삭제 가능하게 함.\"}],\"\\n\"]}]\n29:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"(ReshapeOp:$res (ConstantOp $arg))\\n  =\u003e\\n(ConstantOp (ReshapeConstant $arg, $res))\\n\"}]}]\n2a:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"이건 Runtime reshape 이 아니라 Compile time에 상수 모양을 바꿔서 끝내는 것이다.\"}],\"\\n\"]}]\n2b:[\"$\",\"p\",null,{\"children\":\"5) 실제 케이스: trivial_reshape\"}]\n2c:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"%1 = toy.reshape(%0\\\\) to tensor\u003c2x1xf64\u003e\\n%2 = toy.reshape(%1\\\\) to tensor\u003c2x1xf64\u003e\\n%3 = toy.reshape(%2\\\\) to tensor\u003c2x1xf64\u003e\\n\"}]}]\n2d:[\"$\",\"p\",null,{\"children\":\"초기 IR은 reshpae이 이렇게 3번 연달아 있었지만\\nDRR과 canonicalizer를 돌리게 되면\"}]\n2e:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"%0 = toy.constant dense\u003c[[1.0],[2.0]]\u003e : tensor\u003c2x1xf64\u003e\\ntoy.print %0\\n\"}]}]\n2f:[\"$\",\"p\",null,{\""])</script><script>self.__next_f.push([1,"children\":\"이런 식으로 reshape이 전부 사라지고, constant 모양만 깔끔하게 맞춰진다.\"}]\n30:[\"$\",\"p\",null,{\"children\":\"=====================================================================\"}]\n31:[\"$\",\"p\",null,{\"children\":\"이번에도 긴글을 정리해 보자면\"}]\n32:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Toy IR 처럼 의미가 살아있는 고수준 op들에 대해, 패턴 매칭으로 '쓸데없는 연산'을 싹 걷어내는 것.\",\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"C++ 방식 : OpRewritePattern 으로 직접 구현하는 것\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"선언형 방식 : DRR(TableGen 룰)로 간단히 정의되었음\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Canonicalizer가 greedy 하게 반복 적용 + Pure 면 dead op 제거까지\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n33:[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"36px 0\"}}]\n34:[\"$\",\"$L35\",null,{}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"36:I[27201,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"IconMark\"]\nc:[[\"$\",\"title\",\"0\",{\"children\":\"MLIR Toy Tutorial Ch 3 에 대하여 - CPIST's blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"High-level language-specific optimization using pattern rewriting system.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/blog/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L36\",\"3\",{}]]\n8:null\n"])</script></body></html>