1:"$Sreact.fragment"
f:I[80852,["/blog/_next/static/chunks/796e69ae18b2784c.js","/blog/_next/static/chunks/631eeae4923b8465.js"],"default"]
10:I[97367,["/blog/_next/static/chunks/ff1a16fafef87110.js","/blog/_next/static/chunks/247eb132b7f7b574.js"],"OutletBoundary"]
11:"$Sreact.suspense"
2:T7c4,%%writefile gemm_debug.cu
#include <cuda_runtime.h>
#include <iostream>

#define TILE_SIZE 16

__global__ void matrixMulTiled(float* A, float* B, float* C, int width) {
    __shared__ float sA[TILE_SIZE][TILE_SIZE];
    __shared__ float sB[TILE_SIZE][TILE_SIZE];

    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int row = blockIdx.y * TILE_SIZE + ty;
    int col = blockIdx.x * TILE_SIZE + tx;

    float val = 0;
    // width가 TILE_SIZE의 배수라고 가정 (512/16 = 32\)
    for (int m = 0; m < (width / TILE_SIZE); ++m) {
        sA[ty][tx] = A[row * width + (m * TILE_SIZE + tx)];
        sB[ty][tx] = B[(m * TILE_SIZE + ty) * width + col];
        __syncthreads();

        for (int k = 0; k < TILE_SIZE; ++k) {
            val += sA[ty][k] * sB[k][tx];
        }
        __syncthreads();
    }
    C[row * width + col] = val;
}

int main() {
    int width = 512;
    size_t size = width * width * sizeof(float);

    float *h_A = (float*)malloc(size);
    float *h_B = (float*)malloc(size);
    float *h_C = (float*)malloc(size);

    for(int i=0; i<width*width; i++) { h_A[i] = 1.0f; h_B[i] = 2.0f; }

    float *d_A, *d_B, *d_C;
    cudaMalloc(&d_A, size);
    cudaMalloc(&d_B, size);
    cudaMalloc(&d_C, size);

    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    dim3 dimBlock(TILE_SIZE, TILE_SIZE);
    dim3 dimGrid(width / TILE_SIZE, width / TILE_SIZE);

    matrixMulTiled<<<dimGrid, dimBlock>>>(d_A, d_B, d_C, width);
    
    // 핵심: 커널 실행 후 즉시 에러 체크
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf("CUDA Kernel Error: %s\n", cudaGetErrorString(err));
    }
    cudaDeviceSynchronize();

    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
    std::cout << "Result C[0]: " << h_C[0] << " (Expected: 1024.0\)" << std::endl;

    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
    free(h_A); free(h_B); free(h_C);
    return 0;
}
3:T8d6,import torch
import triton
import triton.language as tl
import os

# MLIR 덤프 활성화 (VSCode Output 창 확인)
os.environ["MLIR_ENABLE_DUMP"] = "1"

@triton.jit
def matmul_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
):
    # 1. 2D 타일 인덱스 계산 (CUDA의 blockIdx와 유사)
    pid = tl.program_id(0\)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n

    # 2. 오프셋 계산 (벡터화된 형태)
    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # 3. 포인터 이동
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

    # 4. 루프를 돌며 타일 연산 (CUDA의 Phase 루프와 동일)
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32\)
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs)
        b = tl.load(b_ptrs)
        # tl.dot이 내부적으로 Shared Memory 활용 및 하드웨어 가속(MMA) 수행
        accumulator += tl.dot(a, b)
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk

    # 5. 결과 저장
    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)
    tl.store(c_ptrs, accumulator)

# 실행 및 검증 함수
def triton_gemm(a, b):
    M, K = a.shape
    K, N = b.shape
    c = torch.empty((M, N), device=a.device, dtype=a.dtype)
    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)
    matmul_kernel[grid](
        a, b, c, M, N, K,
        a.stride(0\), a.stride(1\), b.stride(0\), b.stride(1\), c.stride(0\), c.stride(1\),
        BLOCK_SIZE_M=32, BLOCK_SIZE_N=32, BLOCK_SIZE_K=32
    )
    return c

# 테스트
a = torch.ones((512, 512\), device='cuda')
b = torch.ones((512, 512\), device='cuda') * 2
res = triton_gemm(a, b)
print(f"Triton Result[0,0]: {res[0,0]} (Expected: 1024\)")
0:{"buildId":"Rx5bBwvk2gP0K4H82T2jS","rsc":["$","$1","c",{"children":[["$","article",null,{"children":[["$","div",null,{"style":{"color":"#666","marginBottom":6},"children":["$","a",null,{"href":"/triton","style":{"textDecoration":"none"},"children":["/","triton"]}]}],["$","h2",null,{"style":{"marginTop":0},"children":"왜 Triton 이지?"}],["$","div",null,{"style":{"color":"#666","marginBottom":18},"children":"2026-01-05"}],["$","p",null,{"style":{"color":"#333"},"children":"Simple compare with code"}],["$","hr",null,{"style":{"border":0,"borderTop":"1px solid #eee","margin":"18px 0"}}],["$","div",null,{"className":"prose","children":[["$","p",null,{"children":"1) 사소한 팁인데 (본인도 colab 잘 안써봤어서..)\n%%writefile (파일명) 같은 방식으로 해당 셀의 내용을 Colab 인스턴스 (remote server) 내에 파일로 저장할 수 있음."}],"\n",["$","p",null,{"children":"2) 그리고 .ipynb 로 저장하면 VSCode에서 extension 깔면 자연스럽게 colab kernel에 연결해서 쓸수 있다는 것."}],"\n",["$","p",null,{"children":"3) 일단 CUDA 코드를 보자"}],"\n",["$","pre",null,{"children":["$","code",null,{"children":"$2"}]}],"\n",["$","p",null,{"children":"Colab에서 수행은 아래와 같이 진행"}],"\n",["$","pre",null,{"children":["$","code",null,{"children":"// 여기서 sm_75를 준 이유는, Colab 환경의 드라이버 버전과 nvcc 컴파일러 버전 간의 미스매치 때문에 발생한 에러를 해결하기 위한 것\n// unsupported toolchain이라는 메시지는 현재 GPU 드라이버가 인식할 수 없는 최신(혹은 너무 구형) PTX 코드를 nvcc가 생성했어서 발생함\n// 이 때문에 Colab의 T4 GPU(CUDA 12.4 드라이버) 환경에 맞춰서 아키텍처 옵션을 명시해주어야함.\n// 이게 바로 sm_75 (Turing 아키텍쳐)\n// 아키텍쳐 옵션 추가로 T4 GPU에 맞는 PTX를 생성하도록 강제\n\n!nvcc -arch=sm_75 gemm_debug.cu -o gemm_debug\n!./gemm_debug\n"}]}],"\n",["$","p",null,{"children":"시간은 2초 정도 걸림.\n(!nvidia-smi 로 사용한 GPU 정보 체크 가능하다는 것)"}],"\n",["$","p",null,{"children":"4) 그 다음 triton 코드를 보자"}],"\n",["$","pre",null,{"children":["$","code",null,{"children":"$3"}]}],"\n","$L4","\n","$L5","\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La"]}],"$Lb","$Lc"]}],["$Ld"],"$Le"]}],"loading":null,"isPartial":false}
4:["$","p",null,{"children":"Pytorch 코드로 그냥 실행하면 됨."}]
5:["$","p",null,{"children":"5) 가벼운 비교로 가보자.\n위에서 CUDA 에 한계로 볼수 있는 지점이 Toolchain 관리인데"}]
6:["$","ul",null,{"children":["\n",["$","li",null,{"children":"CUDA 코드는 특정 GPU 아키텍처에 맞게 컴파일된 바이너리(Cubin)나 중간 표현(PTX)을 생성해서"}],"\n",["$","li",null,{"children":"드라이버가 조금만 구형이거나 컴파일러가 너무 앞서가면 실행조차 되지 않음."}],"\n"]}]
7:["$","p",null,{"children":"반면 MLIR/TRITON의 경우"}]
8:["$","ul",null,{"children":["\n",["$","li",null,{"children":"실행 시점에 현재 GPU의 정보를 읽어와서 그에 맞는 MLIR Dialect를 생성하고 최적화하고"}],"\n",["$","li",null,{"children":"사용자가 아키텍쳐 옵션같은거 몰라도 Triton 컴파일러 하위의 Back-end Pass가 이를 알아서 처리함."}],"\n"]}]
9:["$","p",null,{"children":"그리고 한눈에 봐도 Triton이나 MLIR은 위 코드에서 main() 함수가 담당하는 데이터 이동 및 메모리 관리 로직을 추상화하여 자동으로 생성해 줌."}]
a:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Simple 한 GEMM (GEneral Matrix Matrix Multiplication) 연산 하나를 돌리기 위해 계산 외적인 인프라 코드를 짜는 데 별 신경을 다 쓰는데"}],"\n",["$","li",null,{"children":"MLIR은 정말 손쉽게 자동화 해서 하드웨어 최적화된 저수준 호출은 컴파일러가 해주는 셈."}],"\n"]}]
b:["$","hr",null,{"style":{"border":0,"borderTop":"1px solid #eee","margin":"36px 0"}}]
c:["$","$Lf",null,{}]
d:["$","script","script-0",{"src":"/blog/_next/static/chunks/631eeae4923b8465.js","async":true}]
e:["$","$L10",null,{"children":["$","$11",null,{"name":"Next.MetadataOutlet","children":"$@12"}]}]
12:null
