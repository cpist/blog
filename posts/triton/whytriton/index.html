<!DOCTYPE html><!--CKR3lKym9UzC6gXDKKSHT--><html lang="ko"><head><meta charSet="utf-8"/><link rel="preconnect" href="/" crossorigin=""/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/blog/_next/static/chunks/743a8f8bdb738f7f.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/blog/_next/static/chunks/2139e000f4b5d584.js"/><script src="/blog/_next/static/chunks/8a8ef77865bda9e6.js" async=""></script><script src="/blog/_next/static/chunks/0ff423a9fcc0186e.js" async=""></script><script src="/blog/_next/static/chunks/88a8688d62cd2814.js" async=""></script><script src="/blog/_next/static/chunks/turbopack-566b9f8f22ac84c4.js" async=""></script><script src="/blog/_next/static/chunks/796e69ae18b2784c.js" async=""></script><script src="/blog/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/blog/_next/static/chunks/247eb132b7f7b574.js" async=""></script><script src="/blog/_next/static/chunks/631eeae4923b8465.js" async=""></script><link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" as="style" crossorigin="anonymous" integrity="sha384-n8MVd4RsNIU0KOVEMVIqhKyMVPsoloXttrTHYUjDkaWaXIhKbMCh2GbqNl2CAPFu"/><title>왜 Triton 이지? - CPIST&#x27;s blog</title><meta name="description" content="Simple compare with code"/><link rel="icon" href="/blog/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0KOVEMVIqhKyMVPsoloXttrTHYUjDkaWaXIhKbMCh2GbqNl2CAPFu" crossorigin="anonymous"/><script src="/blog/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><main><header style="display:flex;justify-content:space-between;align-items:baseline;gap:12px"><h1 style="margin:8px 0"><a style="text-decoration:none" href="/blog/">CPIST&#x27;s blog</a></h1><nav style="display:flex;gap:12px"><a href="/blog/posts/">Posts</a></nav></header><hr style="border:0;border-top:1px solid #eee;margin:12px 0 24px"/><article style="max-width:800px;margin:0 auto;padding:20px"><div style="color:#666;margin-bottom:6px"><a href="/triton" style="text-decoration:none">/<!-- -->triton</a></div><h2 style="margin-top:0">왜 Triton 이지?</h2><div style="color:#666;margin-bottom:18px">2026-01-05</div><p style="color:#333;font-style:italic">Simple compare with code</p><hr style="border:0;border-top:1px solid #eee;margin:18px 0"/><div class="prose" style="line-height:1.6"><ol>
<li>
<p>사소한 팁인데 (본인도 colab 잘 안써봤어서..)
%%writefile (파일명) 같은 방식으로 해당 셀의 내용을 Colab 인스턴스 (remote server) 내에 파일로 저장할 수 있음.</p>
</li>
<li>
<p>그리고 .ipynb 로 저장하면 VSCode에서 extension 깔면 자연스럽게 colab kernel에 연결해서 쓸수 있다는 것.</p>
</li>
<li>
<p>일단 CUDA 코드를 보자</p>
</li>
</ol>
<pre><code>%%writefile gemm_debug.cu
#include &lt;cuda_runtime.h&gt;
#include &lt;iostream&gt;

#define TILE_SIZE 16

__global__ void matrixMulTiled(float* A, float* B, float* C, int width) {
    __shared__ float sA[TILE_SIZE][TILE_SIZE];
    __shared__ float sB[TILE_SIZE][TILE_SIZE];

    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int row = blockIdx.y * TILE_SIZE + ty;
    int col = blockIdx.x * TILE_SIZE + tx;

    float val = 0;
    // width가 TILE_SIZE의 배수라고 가정 (512/16 = 32)
    for (int m = 0; m &lt; (width / TILE_SIZE); ++m) {
        sA[ty][tx] = A[row * width + (m * TILE_SIZE + tx)];
        sB[ty][tx] = B[(m * TILE_SIZE + ty) * width + col];
        __syncthreads();

        for (int k = 0; k &lt; TILE_SIZE; ++k) {
            val += sA[ty][k] * sB[k][tx];
        }
        __syncthreads();
    }
    C[row * width + col] = val;
}

int main() {
    int width = 512;
    size_t size = width * width * sizeof(float);

    float *h_A = (float*)malloc(size);
    float *h_B = (float*)malloc(size);
    float *h_C = (float*)malloc(size);

    for(int i=0; i&lt;width*width; i++) { h_A[i] = 1.0f; h_B[i] = 2.0f; }

    float *d_A, *d_B, *d_C;
    cudaMalloc(&amp;d_A, size);
    cudaMalloc(&amp;d_B, size);
    cudaMalloc(&amp;d_C, size);

    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    dim3 dimBlock(TILE_SIZE, TILE_SIZE);
    dim3 dimGrid(width / TILE_SIZE, width / TILE_SIZE);

    matrixMulTiled&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C, width);
    
    // 핵심: 커널 실행 후 즉시 에러 체크
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf(&quot;CUDA Kernel Error: %s\n&quot;, cudaGetErrorString(err));
    }
    cudaDeviceSynchronize();

    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
    std::cout &lt;&lt; &quot;Result C[0]: &quot; &lt;&lt; h_C[0] &lt;&lt; &quot; (Expected: 1024.0)&quot; &lt;&lt; std::endl;

    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
    free(h_A); free(h_B); free(h_C);
    return 0;
}
</code></pre>
<p>Colab에서 수행은 아래와 같이 진행</p>
<pre><code>// 여기서 sm_75를 준 이유는, Colab 환경의 드라이버 버전과 nvcc 컴파일러 버전 간의 미스매치 때문에 발생한 에러를 해결하기 위한 것
// unsupported toolchain이라는 메시지는 현재 GPU 드라이버가 인식할 수 없는 최신(혹은 너무 구형) PTX 코드를 nvcc가 생성했어서 발생함
// 이 때문에 Colab의 T4 GPU(CUDA 12.4 드라이버) 환경에 맞춰서 아키텍처 옵션을 명시해주어야함.
// 이게 바로 sm_75 (Turing 아키텍쳐)
// 아키텍쳐 옵션 추가로 T4 GPU에 맞는 PTX를 생성하도록 강제

!nvcc -arch=sm_75 gemm_debug.cu -o gemm_debug
!./gemm_debug
</code></pre>
<p>시간은 2초 정도 걸림.
(!nvidia-smi 로 사용한 GPU 정보 체크 가능하다는 것)</p>
<ol start="4">
<li>그 다음 triton 코드를 보자</li>
</ol>
<pre><code>import torch
import triton
import triton.language as tl
import os

# MLIR 덤프 활성화 (VSCode Output 창 확인)
os.environ[&quot;MLIR_ENABLE_DUMP&quot;] = &quot;1&quot;

@triton.jit
def matmul_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,
):
    # 1. 2D 타일 인덱스 계산 (CUDA의 blockIdx와 유사)
    pid = tl.program_id(0)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n

    # 2. 오프셋 계산 (벡터화된 형태)
    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    # 3. 포인터 이동
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

    # 4. 루프를 돌며 타일 연산 (CUDA의 Phase 루프와 동일)
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs)
        b = tl.load(b_ptrs)
        # tl.dot이 내부적으로 Shared Memory 활용 및 하드웨어 가속(MMA) 수행
        accumulator += tl.dot(a, b)
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk

    # 5. 결과 저장
    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)
    tl.store(c_ptrs, accumulator)

# 실행 및 검증 함수
def triton_gemm(a, b):
    M, K = a.shape
    K, N = b.shape
    c = torch.empty((M, N), device=a.device, dtype=a.dtype)
    grid = lambda META: (triton.cdiv(M, META[&#x27;BLOCK_SIZE_M&#x27;]) * triton.cdiv(N, META[&#x27;BLOCK_SIZE_N&#x27;]),)
    matmul_kernel[grid](
        a, b, c, M, N, K,
        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),
        BLOCK_SIZE_M=32, BLOCK_SIZE_N=32, BLOCK_SIZE_K=32
    )
    return c

# 테스트
a = torch.ones((512, 512), device=&#x27;cuda&#x27;)
b = torch.ones((512, 512), device=&#x27;cuda&#x27;) * 2
res = triton_gemm(a, b)
print(f&quot;Triton Result[0,0]: {res[0,0]} (Expected: 1024)&quot;)
</code></pre>
<p>Pytorch 코드로 그냥 실행하면 됨.</p>
<ol start="5">
<li>가벼운 비교로 가보자.
위에서 CUDA 에 한계로 볼수 있는 지점이 Toolchain 관리인데</li>
</ol>
<ul>
<li>CUDA 코드는 특정 GPU 아키텍처에 맞게 컴파일된 바이너리(Cubin)나 중간 표현(PTX)을 생성해서</li>
<li>드라이버가 조금만 구형이거나 컴파일러가 너무 앞서가면 실행조차 되지 않음.</li>
</ul>
<p>반면 MLIR/TRITON의 경우</p>
<ul>
<li>실행 시점에 현재 GPU의 정보를 읽어와서 그에 맞는 MLIR Dialect를 생성하고 최적화하고</li>
<li>사용자가 아키텍쳐 옵션같은거 몰라도 Triton 컴파일러 하위의 Back-end Pass가 이를 알아서 처리함.</li>
</ul>
<p>그리고 한눈에 봐도 Triton이나 MLIR은 위 코드에서 main() 함수가 담당하는 데이터 이동 및 메모리 관리 로직을 추상화하여 자동으로 생성해 줌.</p>
<ul>
<li>Simple 한 GEMM (GEneral Matrix Matrix Multiplication) 연산 하나를 돌리기 위해 계산 외적인 인프라 코드를 짜는 데 별 신경을 다 쓰는데</li>
<li>MLIR은 정말 손쉽게 자동화 해서 하드웨어 최적화된 저수준 호출은 컴파일러가 해주는 셈.</li>
</ul></div><hr style="border:0;border-top:1px solid #eee;margin:36px 0"/><div></div></article><!--$--><!--/$--><footer style="margin-top:48px;padding-top:16px;border-top:1px solid #eee;color:#666">© <!-- -->2026<!-- --> CPIST&#x27;s blog</footer></main><script src="/blog/_next/static/chunks/2139e000f4b5d584.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[22016,[\"/blog/_next/static/chunks/796e69ae18b2784c.js\"],\"\"]\n3:I[39756,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n4:I[37457,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n6:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"OutletBoundary\"]\n7:\"$Sreact.suspense\"\n9:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"ViewportBoundary\"]\nb:I[97367,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"MetadataBoundary\"]\nd:I[68027,[],\"default\"]\n:HL[\"/blog/_next/static/chunks/743a8f8bdb738f7f.css\",\"style\"]\n:HC[\"/\",\"\"]\n:HL[\"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css\",\"style\",{\"crossOrigin\":\"anonymous\",\"integrity\":\"sha384-n8MVd4RsNIU0KOVEMVIqhKyMVPsoloXttrTHYUjDkaWaXIhKbMCh2GbqNl2CAPFu\"}]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"CKR3lKym9UzC6gXDKKSHT\",\"c\":[\"\",\"posts\",\"triton\",\"whytriton\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"posts\",{\"children\":[[\"slug\",\"triton/whytriton\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/blog/_next/static/chunks/743a8f8bdb738f7f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/blog/_next/static/chunks/796e69ae18b2784c.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"ko\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css\",\"integrity\":\"sha384-n8MVd4RsNIU0KOVEMVIqhKyMVPsoloXttrTHYUjDkaWaXIhKbMCh2GbqNl2CAPFu\",\"crossOrigin\":\"anonymous\"}]}],[\"$\",\"body\",null,{\"children\":[\"$\",\"main\",null,{\"children\":[[\"$\",\"header\",null,{\"style\":{\"display\":\"flex\",\"justifyContent\":\"space-between\",\"alignItems\":\"baseline\",\"gap\":12},\"children\":[[\"$\",\"h1\",null,{\"style\":{\"margin\":\"8px 0\"},\"children\":[\"$\",\"$L2\",null,{\"href\":\"/\",\"style\":{\"textDecoration\":\"none\"},\"children\":\"CPIST's blog\"}]}],[\"$\",\"nav\",null,{\"style\":{\"display\":\"flex\",\"gap\":12},\"children\":[\"$\",\"$L2\",null,{\"href\":\"/posts\",\"children\":\"Posts\"}]}]]}],[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"12px 0 24px\"}}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"style\":{\"marginTop\":48,\"paddingTop\":16,\"borderTop\":\"1px solid #eee\",\"color\":\"#666\"},\"children\":[\"© \",2026,\" CPIST's blog\"]}]]}]}]]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/blog/_next/static/chunks/631eeae4923b8465.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L6\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@8\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L9\",null,{\"children\":\"$@a\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@c\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"e:T7c2,"])</script><script>self.__next_f.push([1,"%%writefile gemm_debug.cu\n#include \u003ccuda_runtime.h\u003e\n#include \u003ciostream\u003e\n\n#define TILE_SIZE 16\n\n__global__ void matrixMulTiled(float* A, float* B, float* C, int width) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    int row = blockIdx.y * TILE_SIZE + ty;\n    int col = blockIdx.x * TILE_SIZE + tx;\n\n    float val = 0;\n    // width가 TILE_SIZE의 배수라고 가정 (512/16 = 32)\n    for (int m = 0; m \u003c (width / TILE_SIZE); ++m) {\n        sA[ty][tx] = A[row * width + (m * TILE_SIZE + tx)];\n        sB[ty][tx] = B[(m * TILE_SIZE + ty) * width + col];\n        __syncthreads();\n\n        for (int k = 0; k \u003c TILE_SIZE; ++k) {\n            val += sA[ty][k] * sB[k][tx];\n        }\n        __syncthreads();\n    }\n    C[row * width + col] = val;\n}\n\nint main() {\n    int width = 512;\n    size_t size = width * width * sizeof(float);\n\n    float *h_A = (float*)malloc(size);\n    float *h_B = (float*)malloc(size);\n    float *h_C = (float*)malloc(size);\n\n    for(int i=0; i\u003cwidth*width; i++) { h_A[i] = 1.0f; h_B[i] = 2.0f; }\n\n    float *d_A, *d_B, *d_C;\n    cudaMalloc(\u0026d_A, size);\n    cudaMalloc(\u0026d_B, size);\n    cudaMalloc(\u0026d_C, size);\n\n    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n\n    dim3 dimBlock(TILE_SIZE, TILE_SIZE);\n    dim3 dimGrid(width / TILE_SIZE, width / TILE_SIZE);\n\n    matrixMulTiled\u003c\u003c\u003cdimGrid, dimBlock\u003e\u003e\u003e(d_A, d_B, d_C, width);\n    \n    // 핵심: 커널 실행 후 즉시 에러 체크\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        printf(\"CUDA Kernel Error: %s\\n\", cudaGetErrorString(err));\n    }\n    cudaDeviceSynchronize();\n\n    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n    std::cout \u003c\u003c \"Result C[0]: \" \u003c\u003c h_C[0] \u003c\u003c \" (Expected: 1024.0)\" \u003c\u003c std::endl;\n\n    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n    free(h_A); free(h_B); free(h_C);\n    return 0;\n}\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"article\",null,{\"style\":{\"maxWidth\":\"800px\",\"margin\":\"0 auto\",\"padding\":\"20px\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"color\":\"#666\",\"marginBottom\":6},\"children\":[\"$\",\"a\",null,{\"href\":\"/triton\",\"style\":{\"textDecoration\":\"none\"},\"children\":[\"/\",\"triton\"]}]}],[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0},\"children\":\"왜 Triton 이지?\"}],[\"$\",\"div\",null,{\"style\":{\"color\":\"#666\",\"marginBottom\":18},\"children\":\"2026-01-05\"}],[\"$\",\"p\",null,{\"style\":{\"color\":\"#333\",\"fontStyle\":\"italic\"},\"children\":\"Simple compare with code\"}],[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"18px 0\"}}],[\"$\",\"div\",null,{\"className\":\"prose\",\"style\":{\"lineHeight\":1.6},\"children\":[[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"사소한 팁인데 (본인도 colab 잘 안써봤어서..)\\n%%writefile (파일명) 같은 방식으로 해당 셀의 내용을 Colab 인스턴스 (remote server) 내에 파일로 저장할 수 있음.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"그리고 .ipynb 로 저장하면 VSCode에서 extension 깔면 자연스럽게 colab kernel에 연결해서 쓸수 있다는 것.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"일단 CUDA 코드를 보자\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"$e\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Colab에서 수행은 아래와 같이 진행\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"// 여기서 sm_75를 준 이유는, Colab 환경의 드라이버 버전과 nvcc 컴파일러 버전 간의 미스매치 때문에 발생한 에러를 해결하기 위한 것\\n// unsupported toolchain이라는 메시지는 현재 GPU 드라이버가 인식할 수 없는 최신(혹은 너무 구형) PTX 코드를 nvcc가 생성했어서 발생함\\n// 이 때문에 Colab의 T4 GPU(CUDA 12.4 드라이버) 환경에 맞춰서 아키텍처 옵션을 명시해주어야함.\\n// 이게 바로 sm_75 (Turing 아키텍쳐)\\n// 아키텍쳐 옵션 추가로 T4 GPU에 맞는 PTX를 생성하도록 강제\\n\\n!nvcc -arch=sm_75 gemm_debug.cu -o gemm_debug\\n!./gemm_debug\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"시간은 2초 정도 걸림.\\n(!nvidia-smi 로 사용한 GPU 정보 체크 가능하다는 것)\"}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\"]}],\"$L18\",\"$L19\"]}]\n"])</script><script>self.__next_f.push([1,"1b:I[80852,[\"/blog/_next/static/chunks/796e69ae18b2784c.js\",\"/blog/_next/static/chunks/631eeae4923b8465.js\"],\"default\"]\nf:[\"$\",\"ol\",null,{\"start\":\"4\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"그 다음 triton 코드를 보자\"}],\"\\n\"]}]\n1a:T8cb,"])</script><script>self.__next_f.push([1,"import torch\nimport triton\nimport triton.language as tl\nimport os\n\n# MLIR 덤프 활성화 (VSCode Output 창 확인)\nos.environ[\"MLIR_ENABLE_DUMP\"] = \"1\"\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    # 1. 2D 타일 인덱스 계산 (CUDA의 blockIdx와 유사)\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    # 2. 오프셋 계산 (벡터화된 형태)\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # 3. 포인터 이동\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # 4. 루프를 돌며 타일 연산 (CUDA의 Phase 루프와 동일)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        # tl.dot이 내부적으로 Shared Memory 활용 및 하드웨어 가속(MMA) 수행\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # 5. 결과 저장\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n    tl.store(c_ptrs, accumulator)\n\n# 실행 및 검증 함수\ndef triton_gemm(a, b):\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    matmul_kernel[grid](\n        a, b, c, M, N, K,\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=32, BLOCK_SIZE_N=32, BLOCK_SIZE_K=32\n    )\n    return c\n\n# 테스트\na = torch.ones((512, 512), device='cuda')\nb = torch.ones((512, 512), device='cuda') * 2\nres = triton_gemm(a, b)\nprint(f\"Triton Result[0,0]: {res[0,0]} (Expected: 1024)\")\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"$1a\"}]}]\n11:[\"$\",\"p\",null,{\"children\":\"Pytorch 코드로 그냥 실행하면 됨.\"}]\n12:[\"$\",\"ol\",null,{\"start\":\"5\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"가벼운 비교로 가보자.\\n위에서 CUDA 에 한계로 볼수 있는 지점이 Toolchain 관리인데\"}],\"\\n\"]}]\n13:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"CUDA 코드는 특정 GPU 아키텍처에 맞게 컴파일된 바이너리(Cubin)나 중간 표현(PTX)을 생성해서\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"드라이버가 조금만 구형이거나 컴파일러가 너무 앞서가면 실행조차 되지 않음.\"}],\"\\n\"]}]\n14:[\"$\",\"p\",null,{\"children\":\"반면 MLIR/TRITON의 경우\"}]\n15:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"실행 시점에 현재 GPU의 정보를 읽어와서 그에 맞는 MLIR Dialect를 생성하고 최적화하고\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"사용자가 아키텍쳐 옵션같은거 몰라도 Triton 컴파일러 하위의 Back-end Pass가 이를 알아서 처리함.\"}],\"\\n\"]}]\n16:[\"$\",\"p\",null,{\"children\":\"그리고 한눈에 봐도 Triton이나 MLIR은 위 코드에서 main() 함수가 담당하는 데이터 이동 및 메모리 관리 로직을 추상화하여 자동으로 생성해 줌.\"}]\n17:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Simple 한 GEMM (GEneral Matrix Matrix Multiplication) 연산 하나를 돌리기 위해 계산 외적인 인프라 코드를 짜는 데 별 신경을 다 쓰는데\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"MLIR은 정말 손쉽게 자동화 해서 하드웨어 최적화된 저수준 호출은 컴파일러가 해주는 셈.\"}],\"\\n\"]}]\n18:[\"$\",\"hr\",null,{\"style\":{\"border\":0,\"borderTop\":\"1px solid #eee\",\"margin\":\"36px 0\"}}]\n19:[\"$\",\"$L1b\",null,{}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"1c:I[27201,[\"/blog/_next/static/chunks/ff1a16fafef87110.js\",\"/blog/_next/static/chunks/247eb132b7f7b574.js\"],\"IconMark\"]\nc:[[\"$\",\"title\",\"0\",{\"children\":\"왜 Triton 이지? - CPIST's blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Simple compare with code\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/blog/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L1c\",\"3\",{}]]\n8:null\n"])</script></body></html>