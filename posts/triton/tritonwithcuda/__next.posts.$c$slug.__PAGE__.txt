1:"$Sreact.fragment"
10:I[80852,["/blog/_next/static/chunks/796e69ae18b2784c.js","/blog/_next/static/chunks/631eeae4923b8465.js"],"default"]
11:I[97367,["/blog/_next/static/chunks/ff1a16fafef87110.js","/blog/_next/static/chunks/247eb132b7f7b574.js"],"OutletBoundary"]
12:"$Sreact.suspense"
0:{"buildId":"IuC2hZvLkyNgf1DyPTwHO","rsc":["$","$1","c",{"children":[["$","article",null,{"children":[["$","div",null,{"style":{"color":"#666","marginBottom":6},"children":["$","a",null,{"href":"/triton","style":{"textDecoration":"none"},"children":["/","triton"]}]}],["$","h2",null,{"style":{"marginTop":0},"children":"Deep dive into Triton with CUDA"}],["$","div",null,{"style":{"color":"#666","marginBottom":18},"children":"2026-01-04"}],["$","p",null,{"style":{"color":"#333"},"children":"Analyze Triton with CUDA, TVM"}],["$","hr",null,{"style":{"border":0,"borderTop":"1px solid #eee","margin":"18px 0"}}],["$","div",null,{"className":"prose","children":[["$","p",null,{"children":"Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations (2019) 는 MAPL 워크숍에서 발표된 논문임.\nTriton 의 구조에 대해 다루었고, 그 이후로도 시간은 많이 흘렀다.\nGPU 컴퓨팅 생태계는 급격하게 변화했는데, **Triton의 접근 방식(Tile-based)**이 결국 업계 표준의 일부가 되었다."}],"\n",["$","p",null,{"children":"1) CUDA의 진화 (Hopper & Blackwell 아키텍처 등장)"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"2019년 (Volta/Ampere): 프로그래머가 스레드 단위로 데이터 이동을 하나하나 코딩해야 했음"}],"\n",["$","li",null,{"children":["2026년 (Hopper/Blackwell): 하드웨어 레벨에서 Triton과 유사한 추상화를 지원하기 시작","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["TMA (Tensor Memory Accelerator): 글로벌 메모리에서 공유 메모리로 데이터를 복사하는 작업을 하드웨어가 비동기적으로 수행","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"이전에는 개발자가 ld.global 명령어로 직접 복사 했음"}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["Thread Block Clusters: 여러 블록이 Distributed Shared Memory를 통해 협력할 수 있게 됨","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"더 거대한 타일링이 가능"}],"\n"]}],"\n"]}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":"즉, CUDA도 하드웨어적으로 데이터 이동의 복잡성을 줄이는 방향으로 발전했으나, 여전히 C++ 기반의 복잡한 문법과 수동 제어가 필요"}],"\n"]}],"\n",["$","p",null,{"children":"2) TVM의 진화 (TVM Unity)"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"2019년: 고수준 그래프(Relay)와 저수준 텐서 표현(TIR)이 분리되어 있어 유연성이 떨어졌었음"}],"\n",["$","li",null,{"children":["2026년 (TVM Unity): **\"Cross-layer interaction\"**을 도입하여 그래프 최적화와 연산자(Operator) 최적화를 한 번에 수행","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"또한 Dynamic Shape(가변 크기 입력) 지원이 대폭 강화"}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":"여전히 커널 작성 난이도는 Triton에 비해 높은 편"}],"\n"]}],"\n",["$","p",null,{"children":"3) Triton의 위상 변화"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"2019년에는 실험적인 연구였지만, OpenAI에 인수된 후 PyTorch 2.0 (torch.compile)의 기본 백엔드가 됨."}],"\n",["$","li",null,{"children":"현재 대부분의 AI 엔지니어는 알게 모르게 Triton을 통해 GPU를 제어"}],"\n"]}],"\n",["$","p",null,{"children":"4) CUDA vs. Triton 비교 분석\nCUDA에서 알아야 할 개념은 Shared Memory, Coalescing, Tiling & Fusion 이다.\n기존 CUDA의 Manual 방식과 Triton의 Compiler-Automated 방식을 비교해 볼 필요가 있음.\n(이건 옛날 2019년의 기준으로 보는게 더 의미 있음)"}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":"Shared Memory (SRAM): 데이터 이동 최소화\n메모리(HBM)에서 데이터를 가져오는 것은 매우 느리기 때문에, 빠른 캐시인 Shared Memory(SRAM)를 잘 쓰는 것이 성능의 핵심"}],"\n"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["CUDA는 Explicit Management (명시적 관리)를 씀","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["개발자가 ",["$","strong",null,{"children":"shared"}]," 키워드로 변수를 선언"]}],"\n",["$","li",null,{"children":"글로벌 메모리에서 데이터를 읽어와(load) 저장(store)하고"}],"\n",["$","li",null,{"children":"다 읽을 때까지 기다리는(__syncthreads()) 코드를 직접 작성"}],"\n",["$","li",null,{"children":"문제점이라면 Bank Conflict(메모리 충돌)를 피하기 위해 패딩(Padding)을 넣는 등 하드웨어 구조를 빠삭하게 알아야 함."}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["반면 Trition은 Automatic Allocation (자동 할당) 을 씀","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"개발자는 \"이 데이터를 쓸 거야\"라고 선언만 하면"}],"\n",["$","li",null,{"children":"컴파일러의 Shared Memory Allocation Pass가 데이터의 **Live Range(생존 구간)**를 분석하고"}],"\n",["$","li",null,{"children":"자동으로 SRAM에 할당하고 관리"}],"\n",["$","li",null,{"children":"컴파일러가 알아서 최적의 위치에 배치하므로, 개발자는 알고리즘 로직에만 집중"}],"\n"]}],"\n"]}],"\n"]}],"\n",["$","ol",null,{"start":"2","children":["\n",["$","li",null,{"children":"Coalesced Memory Access: 메모리 대역폭 낭비 방지\nGPU는 한 번에 32개(Warp)의 스레드가 메모리에 접근\n이때 32개의 주소가 연속적이지 않으면(흩어져 있으면), 메모리 요청을 여러 번 보내야 해서 속도가 급격히 느려짐."}],"\n"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["CUDA는 Manual Indexing (수동 인덱스 계산)을 사용함.","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["개발자가 스레드 ID(threadIdx.x)를 계산할 때, 인접한 스레드가 인접한 메모리 주소(k, k+1, k+2...)를 읽도록 수식을 아주 정교하게 짜야함","\n","$L2","\n"]}],"\n","$L3","\n"]}],"\n"]}],"\n","$L4","\n"]}],"\n","$L5","\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La","\n","$Lb"]}],"$Lc","$Ld"]}],["$Le"],"$Lf"]}],"loading":null,"isPartial":false}
2:["$","ul",null,{"children":["\n",["$","li",null,{"children":"예시로 row-major vs col-major 고려할 때"}],"\n"]}]
3:["$","li",null,{"children":"실수로 인덱스 계산을 잘못하면 성능이 1/10로 떨어진다.."}]
4:["$","li",null,{"children":["Triton은 Automatic Thread Reordering (자동 스레드 재배치)","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"개발자는 그냥 \"블록 단위\" (load(ptr + range))로 코드를 짬."}],"\n",["$","li",null,{"children":"Triton 컴파일러 백엔드가 Memory Coalescing Pass를 통해"}],"\n",["$","li",null,{"children":"실제 GPU 스레드가 실행될 순서를 내부적으로 뒤바꿔서(reorder) 메모리 접근을 묶어줌."}],"\n",["$","li",null,{"children":"논리적인 블록 구조만 잘 정의하면, 물리적인 접근 최적화는 컴파일러가 담당하는 구조"}],"\n"]}],"\n"]}]
5:["$","ol",null,{"start":"3","children":["\n",["$","li",null,{"children":"Tiling & Fusion: 거대한 행렬 쪼개기 및 합치기\n거대한 행렬 곱셈이나 Convolution을 할 때, 한 번에 처리할 수 없으므로 작은 타일(Tile)로 쪼개야 하고\n여러 연산(곱하기+더하기+ReLU)을 하나로 합쳐야(Fusion) 메모리 갔다 오는 횟수를 줄일 수 있음."}],"\n"]}]
6:["$","ul",null,{"children":["\n",["$","li",null,{"children":["CUDA는 Manual Loops & Kernel Fusion 을 사용함.","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"타일링을 위해 3~4중 for 루프를 작성하고, 경계 조건(if (idx < N))을 일일이 검사"}],"\n",["$","li",null,{"children":"Fusion을 하려면 MatMul 커널과 ReLU 커널을 합친 새로운 C++ 커널을 다시 짜야 합니다(Monolithic Kernel)"}],"\n",["$","li",null,{"children":["최적화를 위해 Hierarchical Tiling(L2 캐시 타일 ",["$","code",null,{"children":"->"}]," L1 캐시 타일 ",["$","code",null,{"children":"->"}]," 레지스터 타일)을 손으로 구현해야 해서 코드가 매우 길어짐"]}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["반면 Triton은 Tile Semantics & Automatic Fusion 을 사용함.","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"언어 차원에서 **Tile(block)**이 기본 타입임"}],"\n",["$","li",null,{"children":"C = dot(A, B) + Bias 처럼 쓰면, 컴파일러가 알아서 루프를 만들고 레지스터단에서 연산을 합쳐버립니다(Kernel Fusion)"}],"\n",["$","li",null,{"children":"논문에서는 Hierarchical Tiling Pass가 이 다단계 타일링 구조를 자동으로 생성한다고 설명"}],"\n"]}],"\n"]}],"\n"]}]
7:["$","p",null,{"children":"5) 그러면 Triton은 짱짱이야?\nTriton 역시 단점과 한계가 있음.\nTriton은 **\"행렬 연산(Matrix Multiplication)과 같이 블록(Tile) 단위로 쪼개지는 연산\"**에 특화되어 있음.\n이 범위를 벗어나면 CUDA보다 불편하거나 비효율적일 수 있음."}]
8:["$","ul",null,{"children":["\n",["$","li",null,{"children":["표현력의 한계 (Expressiveness)","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"불규칙한 데이터 접근: 희소 행렬(Sparse Matrix)이나 그래프 알고리즘(Graph Traversal)처럼 메모리 접근이 불규칙한 알고리즘은 Triton의 '블록' 구조로 표현하기 매우 어려움"}],"\n",["$","li",null,{"children":["복잡한 제어 흐름: 스레드마다 제어 흐름이 제각각인(Divergent Control Flow) 복잡한 로직은 CUDA로 짜는 것이 훨씬 직관적이고 효율적","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Triton은 구조적으로 이를 단순화(block 단위 처리)하려고 하기 때문"}],"\n"]}],"\n"]}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["미세 제어 불가 (Lack of Fine-grained Control)","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"레지스터 레벨 튜닝: CUDA 장인들은 레지스터 할당이나 명령어 스케줄링(Instruction Scheduling)까지 수동으로 조절해 극한의 성능을 쥐어짬."}],"\n",["$","li",null,{"children":"Triton은 이를 컴파일러에게 맡기므로, 컴파일러가 최적의 답을 찾지 못하면 사람이 개입해서 수정하기가 어려움."}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["디버깅과 프로파일링","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"CUDA는 지난 15년 넘게 쌓인 방대한 디버깅 툴(Nsight Systems, Nsight Compute 등)과 생태계가 있음"}],"\n",["$","li",null,{"children":"Triton은 생성된 코드(PTX/LLVM IR)를 디버깅해야 하는 경우가 많아, 문제 발생 시 원인을 찾기가 더 까다로울 수 있음."}],"\n"]}],"\n"]}],"\n"]}]
9:["$","p",null,{"children":"6) 그래서 용도별로 다르게 써야 한다."}]
a:["$","ul",null,{"children":["\n",["$","li",null,{"children":["Triton 은 AI 커널 개발 (PyTorch) 사용함","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"적은 코드로 고성능 달성 (가성비 최고)"}],"\n",["$","li",null,{"children":"블록 연산 외에는 구현 난해하다는 건 문제"}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["CUDA는 말 그대로 범용 GPU 프로그래밍의 끝판왕","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"하드웨어 제어권 100%, 모든 기능 구현 가능"}],"\n",["$","li",null,{"children":"배우기 어렵고 코드 작성이 김"}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["TVM은 다양한 하드웨어 배포에 포커싱이 맞춰짐.","\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"다양한 칩(NPU, ARM, DSP 등) 지원"}],"\n",["$","li",null,{"children":"설정과 최적화 과정이 복잡할 수 있음"}],"\n"]}],"\n"]}],"\n"]}]
b:["$","p",null,{"children":"Triton 은 물리 시뮬레이션, 렌더링 엔진(게임), 암호화폐 채굴 등 전통적인 병렬 컴퓨팅 분야에서는 못쓴다는 것이다."}]
c:["$","hr",null,{"style":{"border":0,"borderTop":"1px solid #eee","margin":"36px 0"}}]
d:["$","$L10",null,{}]
e:["$","script","script-0",{"src":"/blog/_next/static/chunks/631eeae4923b8465.js","async":true}]
f:["$","$L11",null,{"children":["$","$12",null,{"name":"Next.MetadataOutlet","children":"$@13"}]}]
13:null
