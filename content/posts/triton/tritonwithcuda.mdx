---
title: "Deep dive into Triton with CUDA"
date: "2026-01-04"
summary: "Analyze Triton with CUDA, TVM"
tags: ["triton", "cuda", "tvm"]
draft: false
---
Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations (2019) 는 MAPL 워크숍에서 발표된 논문임.
Triton 의 구조에 대해 다루었고, 그 이후로도 시간은 많이 흘렀다.
GPU 컴퓨팅 생태계는 급격하게 변화했는데, **Triton의 접근 방식(Tile-based)**이 결국 업계 표준의 일부가 되었다.

1) CUDA의 진화 (Hopper & Blackwell 아키텍처 등장)
    - 2019년 (Volta/Ampere): 프로그래머가 스레드 단위로 데이터 이동을 하나하나 코딩해야 했음
    - 2026년 (Hopper/Blackwell): 하드웨어 레벨에서 Triton과 유사한 추상화를 지원하기 시작
        - TMA (Tensor Memory Accelerator): 글로벌 메모리에서 공유 메모리로 데이터를 복사하는 작업을 하드웨어가 비동기적으로 수행
            - 이전에는 개발자가 ld.global 명령어로 직접 복사 했음
        - Thread Block Clusters: 여러 블록이 Distributed Shared Memory를 통해 협력할 수 있게 됨
            - 더 거대한 타일링이 가능
    - 즉, CUDA도 하드웨어적으로 데이터 이동의 복잡성을 줄이는 방향으로 발전했으나, 여전히 C++ 기반의 복잡한 문법과 수동 제어가 필요

2) TVM의 진화 (TVM Unity)
    - 2019년: 고수준 그래프(Relay)와 저수준 텐서 표현(TIR)이 분리되어 있어 유연성이 떨어졌었음
    - 2026년 (TVM Unity): **"Cross-layer interaction"**을 도입하여 그래프 최적화와 연산자(Operator) 최적화를 한 번에 수행
        - 또한 Dynamic Shape(가변 크기 입력) 지원이 대폭 강화
    - 여전히 커널 작성 난이도는 Triton에 비해 높은 편

3) Triton의 위상 변화
    - 2019년에는 실험적인 연구였지만, OpenAI에 인수된 후 PyTorch 2.0 (torch.compile)의 기본 백엔드가 됨.
    - 현재 대부분의 AI 엔지니어는 알게 모르게 Triton을 통해 GPU를 제어

4) CUDA vs. Triton 비교 분석
CUDA에서 알아야 할 개념은 Shared Memory, Coalescing, Tiling & Fusion 이다.
기존 CUDA의 Manual 방식과 Triton의 Compiler-Automated 방식을 비교해 볼 필요가 있음.
(이건 옛날 2019년의 기준으로 보는게 더 의미 있음)

1. Shared Memory (SRAM): 데이터 이동 최소화
메모리(HBM)에서 데이터를 가져오는 것은 매우 느리기 때문에, 빠른 캐시인 Shared Memory(SRAM)를 잘 쓰는 것이 성능의 핵심

- CUDA는 Explicit Management (명시적 관리)를 씀
    - 개발자가 __shared__ 키워드로 변수를 선언
    - 글로벌 메모리에서 데이터를 읽어와(load) 저장(store)하고
    - 다 읽을 때까지 기다리는(__syncthreads()) 코드를 직접 작성
    - 문제점이라면 Bank Conflict(메모리 충돌)를 피하기 위해 패딩(Padding)을 넣는 등 하드웨어 구조를 빠삭하게 알아야 함.
- 반면 Trition은 Automatic Allocation (자동 할당) 을 씀
    - 개발자는 "이 데이터를 쓸 거야"라고 선언만 하면
    - 컴파일러의 Shared Memory Allocation Pass가 데이터의 **Live Range(생존 구간)**를 분석하고
    - 자동으로 SRAM에 할당하고 관리
    - 컴파일러가 알아서 최적의 위치에 배치하므로, 개발자는 알고리즘 로직에만 집중

2. Coalesced Memory Access: 메모리 대역폭 낭비 방지
GPU는 한 번에 32개(Warp)의 스레드가 메모리에 접근
이때 32개의 주소가 연속적이지 않으면(흩어져 있으면), 메모리 요청을 여러 번 보내야 해서 속도가 급격히 느려짐.

- CUDA는 Manual Indexing (수동 인덱스 계산)을 사용함.
    - 개발자가 스레드 ID(threadIdx.x)를 계산할 때, 인접한 스레드가 인접한 메모리 주소(k, k+1, k+2...)를 읽도록 수식을 아주 정교하게 짜야함
        - 예시로 row-major vs col-major 고려할 때
    - 실수로 인덱스 계산을 잘못하면 성능이 1/10로 떨어진다..
- Triton은 Automatic Thread Reordering (자동 스레드 재배치)
    - 개발자는 그냥 "블록 단위" (load(ptr + range))로 코드를 짬.
    - Triton 컴파일러 백엔드가 Memory Coalescing Pass를 통해 
    - 실제 GPU 스레드가 실행될 순서를 내부적으로 뒤바꿔서(reorder) 메모리 접근을 묶어줌.
    - 논리적인 블록 구조만 잘 정의하면, 물리적인 접근 최적화는 컴파일러가 담당하는 구조

3. Tiling & Fusion: 거대한 행렬 쪼개기 및 합치기
거대한 행렬 곱셈이나 Convolution을 할 때, 한 번에 처리할 수 없으므로 작은 타일(Tile)로 쪼개야 하고
여러 연산(곱하기+더하기+ReLU)을 하나로 합쳐야(Fusion) 메모리 갔다 오는 횟수를 줄일 수 있음.

- CUDA는 Manual Loops & Kernel Fusion 을 사용함.
    - 타일링을 위해 3~4중 for 루프를 작성하고, 경계 조건(if (idx < N))을 일일이 검사
    - Fusion을 하려면 MatMul 커널과 ReLU 커널을 합친 새로운 C++ 커널을 다시 짜야 합니다(Monolithic Kernel)
    - 최적화를 위해 Hierarchical Tiling(L2 캐시 타일 `->` L1 캐시 타일 `->` 레지스터 타일)을 손으로 구현해야 해서 코드가 매우 길어짐
- 반면 Triton은 Tile Semantics & Automatic Fusion 을 사용함.
    - 언어 차원에서 **Tile(block)**이 기본 타입임
    - C = dot(A, B) + Bias 처럼 쓰면, 컴파일러가 알아서 루프를 만들고 레지스터단에서 연산을 합쳐버립니다(Kernel Fusion)
    - 논문에서는 Hierarchical Tiling Pass가 이 다단계 타일링 구조를 자동으로 생성한다고 설명

5) 그러면 Triton은 짱짱이야?
Triton 역시 단점과 한계가 있음.
Triton은 **"행렬 연산(Matrix Multiplication)과 같이 블록(Tile) 단위로 쪼개지는 연산"**에 특화되어 있음.
이 범위를 벗어나면 CUDA보다 불편하거나 비효율적일 수 있음.

- 표현력의 한계 (Expressiveness)
    - 불규칙한 데이터 접근: 희소 행렬(Sparse Matrix)이나 그래프 알고리즘(Graph Traversal)처럼 메모리 접근이 불규칙한 알고리즘은 Triton의 '블록' 구조로 표현하기 매우 어려움
    - 복잡한 제어 흐름: 스레드마다 제어 흐름이 제각각인(Divergent Control Flow) 복잡한 로직은 CUDA로 짜는 것이 훨씬 직관적이고 효율적
        - Triton은 구조적으로 이를 단순화(block 단위 처리)하려고 하기 때문
- 미세 제어 불가 (Lack of Fine-grained Control)
    - 레지스터 레벨 튜닝: CUDA 장인들은 레지스터 할당이나 명령어 스케줄링(Instruction Scheduling)까지 수동으로 조절해 극한의 성능을 쥐어짬.
    - Triton은 이를 컴파일러에게 맡기므로, 컴파일러가 최적의 답을 찾지 못하면 사람이 개입해서 수정하기가 어려움.
- 디버깅과 프로파일링
    - CUDA는 지난 15년 넘게 쌓인 방대한 디버깅 툴(Nsight Systems, Nsight Compute 등)과 생태계가 있음
    - Triton은 생성된 코드(PTX/LLVM IR)를 디버깅해야 하는 경우가 많아, 문제 발생 시 원인을 찾기가 더 까다로울 수 있음.

6) 그래서 용도별로 다르게 써야 한다.
- Triton 은 AI 커널 개발 (PyTorch) 사용함
    - 적은 코드로 고성능 달성 (가성비 최고)
    - 블록 연산 외에는 구현 난해하다는 건 문제
- CUDA는 말 그대로 범용 GPU 프로그래밍의 끝판왕
    - 하드웨어 제어권 100%, 모든 기능 구현 가능
    - 배우기 어렵고 코드 작성이 김
- TVM은 다양한 하드웨어 배포에 포커싱이 맞춰짐.
    - 다양한 칩(NPU, ARM, DSP 등) 지원
    - 설정과 최적화 과정이 복잡할 수 있음

Triton 은 물리 시뮬레이션, 렌더링 엔진(게임), 암호화폐 채굴 등 전통적인 병렬 컴퓨팅 분야에서는 못쓴다는 것이다.


