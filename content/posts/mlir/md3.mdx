---
title: "MLIR Tutorial md3 에 대하여"
date: "2025-11-01"
summary: "High-level language-specific optimization using pattern rewriting system."
tags: ["mlir", "dialect"]
draft: false
---

1) 왜 고수준 dialect인가?
LLVM IR로 내려가면 많은 게 루프와 메모리 load / store로 풀림.
- 당연히 이런 구조면 transpose(transpose(x)) 같은 걸 알아 보기 어렵고
- 임시 배열, 중간 버퍼가 끼면 더더욱 힘들어짐.
반면 MLIR의 Toy IR은 toy.transpose 같은 의미 있는 op로 남아 있어서
- 연산 의미를 그대로 보고
- 간단한 규칙으로 최적화 할 수 있다.

2) 로컬 최적화는 패턴 매칭과 치환에 있다.
컴파일러 변환은 크게 두가지로 나뉨
- Local : 가까운 몇 개 op만 보고 바꾸는 최적화 
- Global : 함수 전체 / 프로그램 전체를 보고 바꾸는 최적화

이번에 다루는 것은 Local인데, '이런 모양이면 이렇게 바꾼다'라는 것을 계속 적용하는 방식이다.
MLIR에는 이를 위한 두가지의 엔진이 있다.
- Generic DAG Rewriter
- 대표 실행 패스가 Canonicalizer (greedy 한 반복 적용 + deadcode 정리)
* 여기서 패스(pass) 라는 개념이 중요하다.
pass는 IR을 한번 훑으면서 분석하거나 변환하는 독립적인 단계를 말함.
즉, IR을 입력으로 받아서 정보를 얻거나 IR을 바꾸고 다음 단계로 넘기는 작업 단위를 말함.
- pass는 IR을 한번 지나간다(pass through)
- 따라서 한 pass는 한번의 IR 통과를 의미하고, 여러 pass 를 연결하면 pass pipeline 이 되는 셈.
- 패스의 이점은 아래와 같음
    - 분리와 재사용 (한 pass는 하나의 책임만, 다른 컴파일러와 언어에서도 재사용 가능)
    - 순서 조절 가능 (최적화 순서 바꾸기 쉽고 실험하기 좋다.)
    - 디버깅 쉬움 (이 패스를 돌린 뒤 IR이 어떻게 변했는지 바로 확인이 가능)
패스에는 크게 2가지로 나뉨. 분석 패스 (Analysis Pass) 와 변환 패스 (Transformation Pass)
- Analysis Pass는 IR을 읽기만 하고, 정보만 계산하는것 
    - 따라서 IR 변경을 안하고
    - 메타정보 (analysis result)를 생성하고
    - 다른 패스가 참고용으로 사용함.
- Transformation Pass 는 IR을 실제로 바꾸는 것
    - 불필요한 연산을 제거한다던지, 연산 순서를 변경 한다던지, 고수준 op -> 저수준 op로 변환 (lowering) 한다던지, 최적화를 한다.
    - 우리가 다루는 Canonicalizer가 대표적인 변환 패스다.
* 여기서 Canonicalizer 패스도 알아볼 필요가 있음.
- 알려진 단순 규칙들을 IR 전체에 반복적으로 적용해서 깔끔하게 만듦.
    - 패턴 매칭 -> Rewrite
    - 더 이상 바뀔 게 없을 때까지 반복
    - dead code 정리를 포함함.
    - 그래서 대표 실행 패스라고 부름.
* 패스는 적용 대상 (scope) 이 있음.
    - ModulePass -> 모듈 전체
    - FunctionPass -> 함수 하나
    - LoopPass -> 루프 하나

```
pm.addNestedPass<toy::FuncOp>(createCanonicalizerPass());
```

이건 모듈 안에 있는 각 toy.func 마다 canonicalizer를 돌리라는 의미임.
- 근데 여기서 어떻게 모듈 단위 / 함수 단위를 알수 있는 걸까?
- PassManager는 ModuleOp 기준으로 만들어진 매니저임.

```
mlir::PassManager pm(module->getName());
```

- MLIR에서 최상위 IR 컨테이너는 항상 ModuleOp 임.
- PassManger 는 기본적으로 ModuleOp에서 실행된다.
- 이 시점에서 패스 파이프라인은 모듈 단위로 시작한다는 것이 정해져 있다는 것.

다시 

```
pm.addNestedPass<toy::FuncOp>(createCanonicalizerPass());

는 대충 아래같은 의미라는 것
runOnModule(module) {
  for (toy::FuncOp f : module.getOps<toy::FuncOp>()) {
    runCanonicalizerOnFunc(f);
  }
}
```

를 보면, ModuleOp 안에 중첩된(nested) toy::FuncOp를 대상으로 이 패스를 실행하라는게 명확해짐.
    - Nested는 최상위(Module) 가 아니라, 그 안쪽에 들어있는 op들을 말하고 (2단계 scope)
    - `<toy::FuncOp>` 는 그 안쪽 op 중에서 타입이 toy::FuncOp 인것을 말한다.

```
ModuleOp
 ├─ toy.func @f1
 ├─ toy.func @f2
 └─ toy.func @f3

여기서 패스를 실행하면

for (auto func : module.getOps<toy::FuncOp>()) {
    run canonicalizer on func;
}
```

즉,
- 모듈을 한번 순회하고
- 그 안에서 각 toy.func를 찾아서 각 함수에 대해 canonicalizer 를 실행하라는 것이다.
- 다시 한번 말하지만 PassManager는 ModuleOp 기준이고, CanonicalizerPass는 FuncOp 기준이다. 
    - 그렇기 때문에 ` addNestedPass<FuncOp> ` 로 연결하는 것이다.
    - 모듈을 기준으로 파이프라인을 시작해서, 그 안에 있는 각 함수 단위로 canonicalizer를 실행한다.

* 만약 모듈단위 패스로 실행하고 싶다면? 

```
pm.addPass(createCanonicalizerPass());
```

* 만약 함수 단위 패스 매니저를 따로 만든다면?

```
OpPassManager &funcPM = pm.nest<toy::FuncOp>();
funcPM.addPass(createCanonicalizerPass());
```

길게 확인해 봤는데 다시 본문으로 돌아오면

3) 예제 1: transpose(transpose(x)) → x (C++ 패턴 리라이터)
Toy 코드는 

```
return transpose(transpose(x));
```

Toy IR은

```
%0 = toy.transpose(%arg0)
%1 = toy.transpose(%0)
return %1
```

- 이걸 C++로도 할수 있음 (OpRewritePattern)
- 여기서 %0는 아무도 안쓰는 거니까, 사용처(use)가 0개인 값을 만드는 셈. 이런 op를 죽은코드(dead) 라고 함.
- Pure trait 라는 것이 있음. op는 부작용이 없고, 같은 입력이면 항상 같은 출력만 만든다는 것.
    - transpose가 대표적인 예인데, 출력 tensor를 계산해서 만들어 내지만
    - 출력 외에 프린트 / 메모리 변경 / IO / 전역상태 변경 같은 부작용이 없음
    - 입력이 같으면 결과도 항상 같다.
- 결과 SSA 값이 안쓰이는, 즉 안쓰이는 경우라면 얘를 제거해도 프로그램 관찰 가능한 결과 (출력, 메모리, 리턴값) 가 바뀌지 않기 때문에 제거해도 의미가 변하지 않음.
- 그래서 지울 수 있음.
- MLIR은 SSA 기반이기 때문에 참조하는 곳이 0개면 그냥 dead value이고, 만약 op가 Pure(부작용 없음) 인 경우 바로 삭제함.
    - 이것이 바로 dead code elimination(DCE) 의 기본 원리임.
다시 Toy case로 돌아와서
- MLIR이 op에 부작용 (side effect) 가 있을지도 몰라 보수적인 판단으로 함부로 못지울 수가 있음.
- 그래서 아래와 같이 Pure trait을 붙이는 것임.

```
def TransposeOp : Toy_Op<"transpose", [Pure]> {...}
```

- MLIR은 transpose 는 부작용이 없기에 -> 안쓰이면 지워도 된다고 판단함.
- 이에 따라 최종 결과가 toy.return %arg0 로 깔끔하게 나옴.

4) reshape 최적화는 DRR(Declarative Rewrite Rules) 로 더 깔끔하게 
Reshape 최적화는 규칙이 많고 반복적이기 떄문에, C++ 보다는 MLIR의 경우 DRR 라는 룰 기반 시스템을 활용함.
우리가 md2에서 ODS로 op를 정의했을때 DRR을 쓸수 있었다.

- DRR Rule example 1: Reshape(Reshape(x)) -> Reshape(x)

```
def ReshapeReshapeOptPattern : Pat<(ReshapeOp(ReshapeOp $arg)),
                                   (ReshapeOp $arg)>;
```

    - sourcePattern: reshape(reshape(arg))
    - resultPattern: reshape(arg) 하나만 남김

- DRR Rule example 2 : 진짜 불필요한 reshape는 제거하기
만약 입력 타입과 출려 타입이 똑같은 경우 reshape는 완전히 쓸모가 없.
따라서 아래와 같이 조건을 추가함.

```
TypesAreIdentical: $0.getType() == $1.getType()
ReshapeOp:$res $arg  =>  replaceWithValue $arg
```

    - reshpae 결과 타입이 입력 타입과 같은 경우
    - reshape op를 없애고 입력을 그대로 쓰게 한다.

- DRR Rule example 3 : Constant reshape는 상수 자체를 바꿔치기 하는 것

reshape(constant) 는
    - constant 의 dense 데이터를 그 shape으로 미리 바꿔서
    - reshape op를 통째로 삭제 가능하게 함.

```
(ReshapeOp:$res (ConstantOp $arg))
  =>
(ConstantOp (ReshapeConstant $arg, $res))
```

    - 이건 Runtime reshape 이 아니라 Compile time에 상수 모양을 바꿔서 끝내는 것이다.

5) 실제 케이스: trivial_reshape

```
%1 = toy.reshape(%0) to tensor<2x1xf64>
%2 = toy.reshape(%1) to tensor<2x1xf64>
%3 = toy.reshape(%2) to tensor<2x1xf64>
```

초기 IR은 reshpae이 이렇게 3번 연달아 있었지만
DRR과 canonicalizer를 돌리게 되면

```
%0 = toy.constant dense<[[1.0],[2.0]]> : tensor<2x1xf64>
toy.print %0
```

이런 식으로 reshape이 전부 사라지고, constant 모양만 깔끔하게 맞춰진다.

=====================================================================

이번에도 긴글을 정리해 보자면

- Toy IR 처럼 의미가 살아있는 고수준 op들에 대해, 패턴 매칭으로 '쓸데없는 연산'을 싹 걷어내는 것.
    - C++ 방식 : OpRewritePattern 으로 직접 구현하는 것
    - 선언형 방식 : DRR(TableGen 룰)로 간단히 정의되었음
    - Canonicalizer가 greedy 하게 반복 적용 + Pure 면 dead op 제거까지







