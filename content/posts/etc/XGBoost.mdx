---
title: "XGBoost"
date: "2026-01-21"
summary: "XGBoost 기법"
tags: ["etc", "kaggle"]
draft: false
---

1) Random Forest 랑 앙상블 기법에서 차이를 보이는게 XGBoost 이다.
- Random Forest는 Bagging을 쓰고, XGBoost는 Boosting을 쓴다.
    - Random Forest는 병렬로 학습을 함, 여러 tree를 동시에 독립적으로 학습을 함.
    - 반면 XGBoost는 순차적으로 학습을 함, 이전 tree의 오차를 다음 tree가 학습한다는 점
- 기본적으로 Random Forest의 목적은 분산 감소, 과적합 방지에 있음
- 반면, XGBoost는 편항 감소, 예측 정확도 향상에 목적이 있음.
    - Random Forest는 전체 데이터에서 중복 허용 샘플링 (Bootstrap) 을 함
    - XGboost 는 오답 (잔차, Residual) 에 가중치를 두어서 샘플링 함
- 최종적으로 Random Forest는 다수결 투표 또는 평균값을 산출함
- 그에 비해, XGBoost는 각 Tree의 예측값을 조금씩 더함.

2) XGBoost의 기본적인 작동 원리를 요약하자면 오차를 줄여나가는 최적화 과정임
- 잔차(Residual) 학습
    - XGBoost는 첫 번째 나무가 예측을 한 뒤, 실제 정답과의 차이인 **'잔차'**를 계산함.
    - 두 번째 나무는 원본 데이터를 학습하는 게 아니라, 이 첫 번째 나무가 틀린 만큼(잔차)을 정답으로 삼아 학습함.
    - $$y = f_1(x) + f_2(x) + f_3(x) + \dots$$
- Gradient Descent (경사 하강법)
    - 이름에 들어가는 'Gradient'는 수학의 미분 개념임. 
    - 모델의 손실 함수(Loss Function)를 최소화하기 위해 미분을 사용하여 어느 방향으로 모델을 업데이트해야 할지 결정함.
- Regularization (규제)
    - XGBoost가 'Extreme'인 이유는 강력한 규제(L1, L2) 기능 때문임.
    - 모델이 너무 복잡해져서 학습 데이터에만 과하게 맞춰지는 것을 수학적으로 방지

3) 언제 쓰나
- Random Forest를 선택할 때
    - 데이터 전처리를 최소화하고 싶을 때 (매우 견고함)
    - 컴퓨팅 자원이 풍부하여 병렬 연산을 빠르게 돌릴 수 있을 때
    - 모델이 학습 데이터에 너무 민감하게 반응하지 않길 원할 때
- XGBoost를 선택할 때
    - 성능(Accuracy)을 극한으로 끌어올려야 할 때
    - 데이터에 결측치(Missing Value)가 많아 모델이 이를 스스로 처리해주길 바랄 때