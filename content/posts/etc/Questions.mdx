---
title: "알아두면 쓸데있는 기초 지식들"
date: "2026-01-03"
summary: "Simple Questions"
tags: ["etc", "tip"]
draft: false
---

1) 툴체인(Toolchain) 이라는게 정확히 무엇일까?
툴체인은 소프트웨어 개발 과정에서 소스 코드가 실행 가능한 바이너리가 되기까지 필요한 **도구들의 연결 고리(Chain)**를 의미
- 컴파일러 하나만을 지칭하는 것이 아닌, 다음의 구성요소들이 유기적으로 엮여 있음.
    - Compiler (Frontend/Backend): 소스 코드를 중간 표현(IR)이나 어셈블리로 변환 (Clang, LLVM 등)
    - Assembler: 어셈블리 코드를 기계어(Object File)로 변환
    - Linker: 여러 개의 오브젝트 파일과 라이브러리를 하나로 묶어 실행 파일 생성 (lld, gold 등)
    - C/C++ Runtime Libraries: libc++, libstdc++ 등 기본 기능을 제공하는 라이브러리
    - Debugger/Profiler: 디버깅과 성능 분석 도구 (LLDB, GDB)
이 도구들이 서로의 출력을 입력으로 받으며 체인처럼 이어져 있기 때문에 툴체인이라고 부름.

2) 프로젝트 정적 검증의 원리
기본적으로 컴파일러 기반의 정적 검증 도구(Clang Static Analyzer 등)는 번역 단위(Translation Unit, TU) 별로 동작함
    - 보통 .c나 .cpp 파일 하나가 하나의 TU가 됨.
    - 개별 검증: 파일 A를 검증할 때 파일 B의 내부 로직은 알지 못하는 것이 기본
    - 전역 검증 (Inter-procedural Analysis): 최근에는 여러 파일 간의 호출 관계를 분석하기 위해 'Cross-Translation Unit (CTU) Analysis' 기능을 사용하기도 한다. 
        - 이 경우 프로젝트 전체의 인덱스를 먼저 생성한 뒤 검증을 수행

* 그렇다면 Import(Include) 파일은 어떻게 처리 할까?
C/C++에서 #include는 **전처리기(Preprocessor)**가 처리
    - 텍스트 치환: 검증 도구가 돌아가기 전, 전처리기는 #include 문을 해당 헤더 파일의 실제 내용으로 통째로 복사해 넣음
    - 거대 파일화: 결과적으로 검증 도구는 헤더 내용이 모두 포함된 하나의 커다란 소스 코드(Preprocessed source)를 읽게 됨
    - 의존성 해결: 따라서 헤더 파일에 선언된 함수나 클래스 정보는 이미 하나의 파일 안에 합쳐진 상태이므로, 검증 도구는 이를 자연스럽게 인식 가능

* 당연한 소리지만, 텍스트 복사에는 한계가 있음
    - C/C++의 #include는 단순히 파일 내용을 복사해서 붙여넣는 방식임.
    - 따라서 헤더 파일이 수십 개 층으로 얽혀 있다면, 전처리가 끝난 후 실제 컴파일러가 읽어야 할 소스 코드(Translation Unit)는 수십만 라인으로 불어나게 됨.
    - 당연히 중복 분석의 낭비가 생김. 
        - 예를 들어 파일 A와 B가 모두 common.h를 포함하고 있다면, 컴파일러는 각 파일을 컴파일할 때마다 common.h를 매번 다시 파싱하고 분석해야 함.
    - 일반적으로는 PCH (Precompiled Headers): 자주 바뀌지 않는 헤더들을 미리 바이너리 형태로 구워놓고 재사용 함.
        - C++20 Modules: #include 대신 import를 사용하여, 한 번 분석된 결과를 바이너리(BMI)로 저장해 두고 재사용함으로써 컴파일 속도를 혁신적으로 줄임.

3) CTU (Cross-Translation Unit) Analysis의 상세 메커니즘
위에서 언급한 전통적인 정적 검증은 "파일 단위"로만 돌아가기 때문에, 다른 파일에 정의된 함수의 리턴값이 무엇인지 알 수 없어 검증에 구멍이 생김.
이를 해결하는 CTU는 보통 다음과 같은 2-Pass 방식으로 동작

1. 인덱싱 및 요약 (Indexing & Summary Generation)
프로젝트 전체를 한 번 훑음.
실제 검증을 수행하는 것이 아니라, 다음 정보를 추출
    - Global Call Graph: 어떤 파일의 어떤 함수가 다른 파일의 무엇을 호출하는지 지도를 그림.
    - Function Summaries: 각 함수의 특징(예: "이 함수는 NULL을 리턴할 수 있음", "이 함수는 메모리를 할당함")을 추상화하여 저장
    - AST Serialization: 각 파일의 추상 구문 트리(AST)를 바이너리 형태로 덤프(Dump)해 둠

2. 실제 검증 (Global Analysis)
이제 각 파일을 검증할 때, 외부 함수 호출을 만나면 미리 덤프해둔 AST를 메모리에 로드하거나 Summary를 참조하여 문맥을 파악

* 참고로 이건 연구 주제임.
    - 전체 AST를 다 불러오면 메모리가 터지고
    - Summary만 쓰자니 정확도가 떨어짐
    - 따라서 **"최소한의 메모리로 최대한의 정확도를 뽑아내는 요약 기법"**과 **"증분 분석(변경된 부분만 분석)"**이 학계와 산업계(LLVM/Clang 커뮤니티 등)의 핵심 화두

4) LLVM과 MLIR의 처리 flow (Flow comparison)
두 기술의 가장 큰 차이점은 **"추상화의 단계(Level of Abstraction)"** 임.

1. [LLVM Flow: 전통적인 3단계 구조]
LLVM은 보통 단일 수준의 IR(LLVM IR)을 중심으로 동작
    - Frontend (Clang): C/C++ 코드를 읽어 구문 분석(AST) 후 LLVM IR로 변환
    - Optimizer (LLVM Opt): 생성된 LLVM IR을 대상으로 하드웨어 독립적인 최적화(Dead code elimination, Loop unrolling 등)를 수행
    - Backend (LLVM CodeGen): LLVM IR을 특정 타겟(x86, ARM, RISC-V 등)의 기계어로 변환

2. [MLIR Flow: 다중 계층 구조]
MLIR은 LLVM IR 하나로 모든 것을 표현하기엔 너무 정보 손실이 크다는 문제에서 출발
    - Frontend: 소스 코드를 읽어 가장 높은 수준의 Dialect(예: Toy, TOSA, Linalg 등)로 변환
    - Progressive Lowering (점진적 하향): * 한 번에 기계어로 가는 게 아니라, High-level Dialect $\rightarrow$ Mid-level Dialect $\rightarrow$ Low-level Dialect 순서로 단계적으로 변환
        - 예: "행렬 곱셈" 연산이 있다면, 이를 처음엔 'Matmul'이라는 고수준 개념으로 최적화하고, 다음 단계에서 'Loop'로 풀고, 마지막에 'LLVM Dialect'로 내림
    - Conversion to LLVM IR: 최종적으로 LLVM Dialect가 된 코드를 LLVM IR로 Export 함.
    - LLVM Backend: 이후는 기존 LLVM flow를 타고 기계어가 생성

* 정리해 보자면
- LLVM은 "모든 언어를 하나의 공통 IR로 모아서 최적화하자"는 주의
- MLIR은 "도메인에 맞는 여러 단계의 IR(Dialect)을 거치며 단계별로 최적화하자"는 주의

5) Coverity 같은 상용툴의 동작 방식
사용 툴들도 정적 검증 원리를 동일하게 사용하긴 하는데, 환경 재현과 분석 엔진에서 상용 툴만의 노하우가 들어감.

1. 빌드 인터셉션 (Build Interception)
- Coverity는 보통 cov-build라는 래퍼(Wrapper)를 사용
    - 사용자가 make나 cmake로 빌드할 때, Coverity가 그 호출을 가로챔
    - 컴파일러 설정 복제: 실제 빌드에 쓰인 컴파일러(gcc, armcc 등)가 무엇인지, 어떤 매크로(-DDEBUG)와 인클루드 경로(-I/usr/include)를 썼는지 그대로 캡처
    - Emit: 캡처한 정보를 바탕으로 소스 코드를 Coverity 전용 중간 언어(Intermediate Representation, 주로 SIR)로 변환하여 로컬 DB에 저장

2. 컴파일러 Configuration이 중요한 이유
- 정적 검증 도구가 코드를 제대로 해석하려면, 실제 타겟 환경의 컴파일러와 똑같이 생각해야 하기 때문
    - int가 16비트인가 32비트 인가
    - 컴파일러가 기본으로 정의해둔 내장 매크로(__linux__ 등)는 무엇인가
        - 이를 맞추지 못하면 정적 검증 도구는 코드를 읽다가 에러(Parse Error)를 내뱉거나 엉뚱한 경로를 분석

3. 상용툴의 경우
- LLVM 기반의 오픈소스 도구보다 훨씬 더 강력한 Global Dataflow Analysis 엔진을 가짐
- 수백만 라인의 코드에서도 메모리 릭이나 레이스 컨디션을 찾아내기 위해 고도로 최적화된 Symbolic Execution 기법을 적용
- 오탐(False Positive)을 줄이기 위한 방대한 체커(Checker) 로직을 보유
